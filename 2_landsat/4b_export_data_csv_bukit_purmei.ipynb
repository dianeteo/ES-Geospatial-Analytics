{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Filtering by specific postal code for spatial map plotting (Bukit Purmei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for postal code 090112: Longitude 103.82593292805574, Latitude 1.2745285256209595\n",
      "Coordinates for postal code 090114: Longitude 103.82588719010951, Latitude 1.2750718182249274\n",
      "Coordinates for postal code 090113: Longitude 103.82693226761857, Latitude 1.2747701258018154\n",
      "Coordinates for postal code 090115: Longitude 103.82695018030107, Latitude 1.2753520132629723\n"
     ]
    }
   ],
   "source": [
    "# Exporting data for blocks of interest and control blocks\n",
    "import geopandas as gpd\n",
    "\n",
    "geojson_path = \"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\ADDRPT.geojson\"\n",
    "postal_code_112 = \"090112\"\n",
    "postal_code_114 = \"090114\"\n",
    "postal_code_113 = \"090113\"\n",
    "postal_code_115 = \"090115\"\n",
    "\n",
    "# Load the GeoJSON file\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Function to retrieve coordinates by postal code\n",
    "def get_coordinates_by_postal_code(postal_code):\n",
    "    # Filter GeoDataFrame for the given postal code\n",
    "    filtered_gdf = gdf[gdf['POSTAL_CODE'] == postal_code]\n",
    "    if not filtered_gdf.empty:\n",
    "        # Extract coordinates\n",
    "        point = filtered_gdf.iloc[0].geometry\n",
    "        return point.x, point.y\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "longitude_112, latitude_112 = get_coordinates_by_postal_code(postal_code_112)\n",
    "longitude_114, latitude_114 = get_coordinates_by_postal_code(postal_code_114)\n",
    "longitude_113, latitude_113 = get_coordinates_by_postal_code(postal_code_113)\n",
    "longitude_115, latitude_115 = get_coordinates_by_postal_code(postal_code_115)\n",
    "\n",
    "if longitude_112 and latitude_112 and longitude_114 and latitude_114:\n",
    "    print(f'Coordinates for postal code {postal_code_112}: Longitude {longitude_112}, Latitude {latitude_112}')\n",
    "    print(f'Coordinates for postal code {postal_code_114}: Longitude {longitude_114}, Latitude {latitude_114}')\n",
    "    print(f'Coordinates for postal code {postal_code_113}: Longitude {longitude_113}, Latitude {latitude_113}')\n",
    "    print(f'Coordinates for postal code {postal_code_115}: Longitude {longitude_115}, Latitude {latitude_115}')\n",
    "else:\n",
    "    print('Postal code not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central coordinates:\n",
      "Longitude: 103.82642564152123, Latitude: 1.2749306207276687\n"
     ]
    }
   ],
   "source": [
    "# Finding the central postal code for Blocks 112, 114, 113 and 115\n",
    "if longitude_112 and latitude_112 and longitude_114 and latitude_114:\n",
    "    # Calculate the average coordinates\n",
    "    avg_longitude = (longitude_112 + longitude_114 + longitude_113 + longitude_115) / 4\n",
    "    avg_latitude = (latitude_112 + latitude_114 + latitude_113 + latitude_115) / 4\n",
    "    print(f'Central coordinates:')\n",
    "    print(f'Longitude: {avg_longitude}, Latitude: {avg_latitude}')\n",
    "else:\n",
    "    print('Postal code not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting x and y to coordinates for latitude/longitude\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import Point\n",
    "\n",
    "global filtered_df\n",
    "\n",
    "def preprocessing(file_path):   \n",
    "    global filtered_df\n",
    "    \n",
    "    # Open your GeoTIFF file\n",
    "    with rasterio.open(file_path) as src:\n",
    "        array = src.read()\n",
    "        transform = src.transform\n",
    "        src_crs = src.crs  # Source CRS\n",
    "        dest_crs = 'EPSG:4326'  # WGS 84\n",
    "\n",
    "        # Create a transformer object to convert from src_crs to dest_crs\n",
    "        transformer = Transformer.from_crs(src_crs, dest_crs, always_xy=True)\n",
    "\n",
    "        # Get arrays of column and row indices\n",
    "        cols, rows = np.meshgrid(np.arange(array.shape[2]), np.arange(array.shape[1]))\n",
    "        \n",
    "        # Convert meshgrid arrays to coordinate arrays using rasterio's method, which are 2D\n",
    "        xs, ys = rasterio.transform.xy(transform, rows, cols, offset='center')\n",
    "        \n",
    "        # Flatten the coordinate arrays to pass to transform function\n",
    "        lon, lat = transformer.transform(np.array(xs).flatten(), np.array(ys).flatten())\n",
    "\n",
    "        # Create DataFrame and convert to GeoDataFrame\n",
    "        df = pd.DataFrame({'Longitude': lon, 'Latitude': lat})\n",
    "        for i, band in enumerate(src.read(masked=True)):\n",
    "            df[src.descriptions[i]] = band.flatten()\n",
    "\n",
    "        # # Convert 'SR_QA_AEROSOL' to integer for bitwise operation\n",
    "        # df['SR_QA_AEROSOL'] = df['SR_QA_AEROSOL'].astype(int)\n",
    "\n",
    "        # # Filter out pixels with valid aerosol retrieval and high aerosol level\n",
    "        # # Assuming 'SR_QA_AEROSOL' is the name of the QA aerosol band in the data\n",
    "        # valid_aerosol = (df['SR_QA_AEROSOL'] & 2) == 2  # Bit 1 must be set for valid retrieval\n",
    "        # high_aerosol = (df['SR_QA_AEROSOL'] & 192) == 192  # Bits 6-7 must be set to 11 for high aerosol\n",
    "        # filter_mask = valid_aerosol & high_aerosol\n",
    "        # df_filtered = df[-filter_mask]\n",
    "\n",
    "        df_filtered = df\n",
    "        \n",
    "        # Scale and offset specific bands\n",
    "        df_filtered['ST_B10_Celsius'] = df_filtered['ST_B10'] * 0.00341802 + 149 - 273.15\n",
    "        df_filtered = df_filtered[df_filtered['ST_B10_Celsius'] >= 20]  # Drop rows below 20 degrees Celsius\n",
    "        \n",
    "        bands_to_scale = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']\n",
    "        for band in bands_to_scale:\n",
    "            df_filtered[f\"{band}_Scaled\"] = df_filtered[band] * 2.75e-05 - 0.2\n",
    "\n",
    "        additional_scales = {\n",
    "            'ST_ATRAN': 0.0001, 'ST_CDIST': 0.01, 'ST_DRAD': 0.001, \n",
    "            'ST_EMIS': 0.0001, 'ST_EMSD': 0.0001, 'ST_QA': 0.01, \n",
    "            'ST_TRAD': 0.001, 'ST_URAD': 0.001\n",
    "        }\n",
    "\n",
    "        for band, scale in additional_scales.items():\n",
    "            df_filtered[f\"{band}_Scaled\"] = df_filtered[band] * scale\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(df_filtered, geometry=gpd.points_from_xy(df_filtered.Longitude, df_filtered.Latitude))\n",
    "        gdf.set_crs('EPSG:4326', inplace=True)  # Ensure the CRS is set to WGS 84\n",
    "\n",
    "        print(\"Total number of valid pixels: \" + str(len(gdf)))\n",
    "        print(df[['Latitude', 'Longitude']].head())\n",
    "\n",
    "        # Define your point of interest and buffer distance in meters\n",
    "        poi = Point(avg_longitude, avg_latitude)\n",
    "        desired_radius = 200\n",
    "        buffer = poi.buffer(desired_radius / 111320)  # Convert meters to degrees approximately\n",
    "\n",
    "        # Filter points within the buffer\n",
    "        filtered_gdf = gdf[gdf.geometry.within(buffer)]\n",
    "\n",
    "        # Save or process your filtered data\n",
    "        print(f\"\\nNumber of points within {desired_radius}m radius: {len(filtered_gdf)}\")\n",
    "        #print(filtered_gdf['ST_B10_Celsius'].head())\n",
    "\n",
    "    return filtered_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining boundaries and plotting region of interest using GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['osm_id', 'osm_type', 'addr_street', 'building', 'name', 'access_roof',\n",
      "       'building_material', 'addr_housenumber', 'roof_material', 'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "geojson_path = \"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\SG_geojson\\\\SG.geojson\"\n",
    "\n",
    "geo_data = gpd.read_file(geojson_path)\n",
    "\n",
    "# Display the matching features\n",
    "print(geo_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYGON ((103.8254384 1.2743995, 103.825484 1.2743955, 103.8254826 1.2743647, 103.8257254 1.274362, 103.8257254 1.2744009, 103.8260325 1.2743915, 103.8260754 1.2744344, 103.8262618 1.274429, 103.8263745 1.2745524, 103.8263785 1.2747656, 103.8262082 1.2747669, 103.8262045 1.274549, 103.8257106 1.2745598, 103.8257079 1.2745108, 103.8254424 1.2745122, 103.8254384 1.2743995))\n",
      "POLYGON ((103.8254451 1.274657, 103.8255926 1.2746516, 103.8255886 1.2750116, 103.8256382 1.2750605, 103.8258664 1.2750513, 103.8258635 1.2749908, 103.8261706 1.2749835, 103.8261733 1.2750663, 103.82622 1.2750652, 103.8262177 1.2749582, 103.826303 1.2749563, 103.8263065 1.2751152, 103.8263989 1.2751132, 103.8264009 1.2752002, 103.8262434 1.2752037, 103.8262454 1.2752961, 103.8261471 1.2752983, 103.8261437 1.2751449, 103.8258625 1.2751533, 103.8258635 1.2751839, 103.8255618 1.2751933, 103.8254545 1.2750927, 103.8254451 1.274657))\n",
      "POLYGON ((103.8264643 1.2746563, 103.8268371 1.2746362, 103.8268412 1.2746898, 103.8270133 1.2746847, 103.827013 1.2747262, 103.8272475 1.274718, 103.8273682 1.2748239, 103.8273759 1.2750803, 103.8272046 1.275086, 103.8271993 1.2748183, 103.8268457 1.27483, 103.8268442 1.2748025, 103.8264737 1.2748159, 103.8264643 1.2746563))\n",
      "POLYGON ((103.8264791 1.2749426, 103.8266159 1.2749426, 103.8266239 1.2752985, 103.8266789 1.2753441, 103.826868 1.2753354, 103.8268653 1.2752871, 103.8270262 1.2752804, 103.8270262 1.2752308, 103.8272086 1.2752228, 103.8272096 1.2752788, 103.8272529 1.2752764, 103.8272489 1.2751624, 103.827336 1.2751598, 103.8273389 1.2753173, 103.8274248 1.2753141, 103.8274277 1.2754222, 103.8272797 1.2754266, 103.8272797 1.2755305, 103.8271832 1.2755338, 103.8271764 1.2753569, 103.8270665 1.2753622, 103.8270665 1.2754165, 103.8268895 1.2754239, 103.8268908 1.2754775, 103.8266011 1.2754815, 103.8264911 1.2753797, 103.8264791 1.2749426))\n"
     ]
    }
   ],
   "source": [
    "blocks_of_interest = ['112', '114', '113', '115']\n",
    "\n",
    "polygons = {}\n",
    "\n",
    "for block in blocks_of_interest:\n",
    "    matching_features = geo_data[(geo_data['addr_street'].str.contains(\"Bukit Purmei\", na=False)) & \n",
    "                                (geo_data['addr_housenumber'].str.contains(f\"{block}\", na=False))]\n",
    "\n",
    "    if not matching_features.empty:\n",
    "        polygon = matching_features.iloc[0]['geometry']\n",
    "        polygons[f'polygon_{block}'] = polygon\n",
    "        print(polygons[f'polygon_{block}'])\n",
    "    else:\n",
    "        print(\"No matching features found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering 30m x 30m pixels based on region of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using ESPG:3857 allows you to blow up the pixels in metres because the coordinate representation is in metres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.2/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='p1002'>\n",
       "  <div id=\"d7782be2-942d-4802-bc8f-355af7c1f740\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"345163e3-a444-4820-91ac-1d7e2bfcad8d\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"p1002\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"p1003\",\"attributes\":{\"plot_id\":\"p1002\",\"comm_id\":\"ad2f80d59bef48e599818719471aa0fc\",\"client_comm_id\":\"cd682b573d6e40759ae3cf550e0ecbfd\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"345163e3-a444-4820-91ac-1d7e2bfcad8d\",\"roots\":{\"p1002\":\"d7782be2-942d-4802-bc8f-355af7c1f740\"},\"root_ids\":[\"p1002\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "p1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon, box\n",
    "import panel as pn\n",
    "from bokeh.palettes import Inferno256\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "logging.getLogger('bokeh').setLevel(logging.ERROR)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "global within_polygon_gdf\n",
    "\n",
    "def plot_spatial_map(filtered_gdf): \n",
    "    global within_polygon_gdf\n",
    "    \n",
    "    filtered_gdf = filtered_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # print(filtered_gdf['geometry'])\n",
    "\n",
    "    # Create pixels as 30m x 30m boxes around each point\n",
    "    # Assuming each point is at the center of the pixel\n",
    "    half_width = 15  # half the width of the pixel in meters since the ESPG:3857 coordinate system is in metres\n",
    "    filtered_gdf['geometry'] = filtered_gdf['geometry'].apply(lambda x: box(x.x - half_width, x.y - half_width, x.x + half_width, x.y + half_width))\n",
    "\n",
    "    #print(filtered_gdf['geometry'])\n",
    "\n",
    "    # Create a GeoDataFrame from all polygons and convert CRS to match\n",
    "    polygon_gdf = gpd.GeoDataFrame({'geometry': list(polygons.values())}, crs='epsg:4326')\n",
    "    polygon_gdf_3857 = polygon_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # Filter points that intersect any polygon\n",
    "    def intersects_any_polygon(point):\n",
    "        return any(point.intersects(poly) for poly in polygon_gdf['geometry'])\n",
    "    \n",
    "    filtered_gdf['intersects'] = filtered_gdf['geometry'].apply(intersects_any_polygon)\n",
    "\n",
    "    # Check intersection with any polygon\n",
    "    within_polygon_gdf = filtered_gdf[filtered_gdf['intersects']].copy()\n",
    "\n",
    "    # print(polygon_gdf_3857['geometry'])\n",
    "\n",
    "    # Filter points that intersect any polygon\n",
    "    filtered_gdf['intersects'] = filtered_gdf['geometry'].apply(\n",
    "        lambda geom: any(geom.intersects(poly) for poly in polygon_gdf_3857['geometry']))\n",
    "    within_polygon_gdf = filtered_gdf[filtered_gdf['intersects']].copy()\n",
    "\n",
    "    print(\"Number of pixels in region of interest: \" + str(len(within_polygon_gdf)))\n",
    "\n",
    "    # Print or use the filtered GeoDataFrame as needed\n",
    "    # print(\"\\nNumber of points within the region of interest: \" + str(len(within_polygon_gdf)))\n",
    "\n",
    "    # # Print the centroids of the intersected pixels\n",
    "    # for index, row in within_polygon_gdf.iterrows():\n",
    "    #     centroid = row['geometry'].centroid\n",
    "    #     print(f\"Longitude: {centroid.x}, Latitude: {centroid.y}\")\n",
    "\n",
    "    # Define a function to select a subset of the color palette\n",
    "    def select_colors(palette, n):\n",
    "        return [palette[int(i)] for i in np.linspace(0, len(palette)-1, n)]\n",
    "\n",
    "    # Create a custom color scale using a continuous palette\n",
    "    custom_palette = select_colors(Inferno256, 256)  # More colors for smoother transitions\n",
    "\n",
    "    # Create the heatmap using the centroid points of intersected pixels\n",
    "    heatmap = within_polygon_gdf.hvplot.points('Longitude', 'Latitude', geo=True, c='ST_B10_Celsius', cmap=custom_palette, size=5, tiles='OSM', frame_width=700, frame_height=500, colorbar=True, clim=(20, 40))\n",
    "\n",
    "    # Plot square polygons with the same color mapping as the points\n",
    "    squares_plot = within_polygon_gdf.hvplot.polygons('geometry', c='ST_B10_Celsius', cmap=custom_palette, alpha=0.5, colorbar=True, clim=(20, 40))\n",
    "\n",
    "    # Plot the polygon with visible settings\n",
    "    polygon_plot = polygon_gdf.hvplot(geo=True, color='red', line_width=3, alpha=0.7)\n",
    "\n",
    "    # Overlay the polygon onto the heatmap\n",
    "    overlay_map = polygon_plot * heatmap * squares_plot\n",
    "\n",
    "    # Set up Panel to display the plot\n",
    "    # pane = pn.panel(overlay_map)\n",
    "\n",
    "    # pane.show()\n",
    "    # pane.save(f'C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\MSE-ES-UHI\\\\2_landsat\\\\Heatmaps\\\\{postal_code_112}_{satellite_image}_LST_Filtered.html', embed=True)\n",
    "\n",
    "    return overlay_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting LST over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining GDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing: L9_UTC_20211031_031441.tif\n",
      "Total number of valid pixels: 1351170\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "Currently processing: L9_UTC_20211105_031711.tif\n",
      "Total number of valid pixels: 496054\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 83\n",
      "Currently processing: L9_UTC_20211110_031939.tif\n",
      "Total number of valid pixels: 0\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "Currently processing: L9_UTC_20211115_032207.tif\n",
      "Total number of valid pixels: 878\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "          Longitude  Latitude   SR_B1   SR_B2    SR_B3   SR_B4    SR_B5  \\\n",
      "1273223  103.825765  1.276485  8273.0  8473.0   9656.0  8397.0  11258.0   \n",
      "1273224  103.826034  1.276485  8517.0  8642.0   9831.0  8712.0  11445.0   \n",
      "1273225  103.826304  1.276485  7923.0  8083.0   9462.0  8218.0  12250.0   \n",
      "1273226  103.826573  1.276485  7783.0  7973.0   9399.0  7944.0  13063.0   \n",
      "1275003  103.825225  1.276213  8197.0  8324.0   9653.0  8374.0  11631.0   \n",
      "...             ...       ...     ...     ...      ...     ...      ...   \n",
      "1292825  103.825766  1.273500  8086.0  8476.0  10059.0  8862.0  11732.0   \n",
      "1292826  103.826035  1.273500  7850.0  8166.0  10052.0  8882.0  12733.0   \n",
      "1292827  103.826305  1.273500  6557.0  7424.0   9922.0  8513.0  14319.0   \n",
      "1294608  103.826036  1.273229  8497.0  8922.0  10503.0  8954.0  13231.0   \n",
      "1294609  103.826305  1.273229  6299.0  7017.0   9605.0  8073.0  14917.0   \n",
      "\n",
      "           SR_B6    SR_B7  SR_QA_AEROSOL  ...  ST_ATRAN_Scaled  \\\n",
      "1273223  10201.0   9676.0          224.0  ...           0.3423   \n",
      "1273224  10858.0  10126.0          194.0  ...           0.3423   \n",
      "1273225  10642.0   9741.0          224.0  ...           0.3423   \n",
      "1273226  10192.0   9186.0          224.0  ...           0.3423   \n",
      "1275003  10496.0   9633.0          224.0  ...           0.3423   \n",
      "...          ...      ...            ...  ...              ...   \n",
      "1292825  11019.0  10110.0          224.0  ...           0.3423   \n",
      "1292826  11960.0  10794.0          224.0  ...           0.3423   \n",
      "1292827  12908.0  11264.0          224.0  ...           0.3423   \n",
      "1294608  11803.0  11101.0          194.0  ...           0.3423   \n",
      "1294609  12543.0  10918.0          224.0  ...           0.3423   \n",
      "\n",
      "         ST_CDIST_Scaled  ST_DRAD_Scaled  ST_EMIS_Scaled  ST_EMSD_Scaled  \\\n",
      "1273223             0.21            2.18          0.9380          0.0000   \n",
      "1273224             0.18            2.18          0.9736          0.0092   \n",
      "1273225             0.15            2.18          0.9755          0.0092   \n",
      "1273226             0.12            2.18          0.9768          0.0092   \n",
      "1275003             0.27            2.18          0.9380          0.0000   \n",
      "...                  ...             ...             ...             ...   \n",
      "1292825             0.21            2.18          0.9774          0.0057   \n",
      "1292826             0.18            2.18          0.9672          0.0128   \n",
      "1292827             0.15            2.18          0.9672          0.0128   \n",
      "1294608             0.18            2.18          0.9672          0.0128   \n",
      "1294609             0.15            2.18          0.9672          0.0128   \n",
      "\n",
      "         ST_QA_Scaled  ST_TRAD_Scaled  ST_URAD_Scaled  \\\n",
      "1273223          6.20        8.066000           5.112   \n",
      "1273224          6.40        8.061001           5.112   \n",
      "1273225          6.55        8.052000           5.112   \n",
      "1273226          6.70        8.039001           5.112   \n",
      "1275003          5.94        8.047000           5.112   \n",
      "...               ...             ...             ...   \n",
      "1292825          6.23        8.087001           5.112   \n",
      "1292826          6.40        8.074000           5.112   \n",
      "1292827          6.57        8.030001           5.112   \n",
      "1294608          6.40        8.062000           5.112   \n",
      "1294609          6.57        8.017000           5.112   \n",
      "\n",
      "                          geometry       time  \n",
      "1273223  POINT (103.82576 1.27648) 2021-11-05  \n",
      "1273224  POINT (103.82603 1.27649) 2021-11-05  \n",
      "1273225  POINT (103.82630 1.27649) 2021-11-05  \n",
      "1273226  POINT (103.82657 1.27649) 2021-11-05  \n",
      "1275003  POINT (103.82523 1.27621) 2021-11-05  \n",
      "...                            ...        ...  \n",
      "1292825  POINT (103.82577 1.27350) 2021-11-05  \n",
      "1292826  POINT (103.82604 1.27350) 2021-11-05  \n",
      "1292827  POINT (103.82631 1.27350) 2021-11-05  \n",
      "1294608  POINT (103.82604 1.27323) 2021-11-05  \n",
      "1294609  POINT (103.82631 1.27323) 2021-11-05  \n",
      "\n",
      "[83 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "# Note that Landsat9 only has data from 2021 onwards\n",
    "year = \"2021\"\n",
    "\n",
    "# Suppress warnings\n",
    "logging.getLogger('bokeh').setLevel(logging.ERROR)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Specify the zip file and temporary directory for extraction\n",
    "zip_file_path = f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\Landsat9\\\\{year}.zip\"\n",
    "temp_dir = f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\temp_extract\"\n",
    "\n",
    "# Create a temporary directory if it doesn't exist\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Extract the .tif files from the zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "# Initialize an empty list to hold all the GeoDataFrames\n",
    "gdfs = []\n",
    "\n",
    "# Walk through the temporary directory and process each .tif file\n",
    "for filename in os.listdir(f\"{temp_dir}\\\\{year}\"):\n",
    "    if filename.endswith(\".tif\"):\n",
    "        print(\"Currently processing: \" + filename)\n",
    "        file_path = os.path.join(f\"{temp_dir}\\\\{year}\", filename)\n",
    "        \n",
    "        # Extract the time period from the filename\n",
    "        # Assuming filename format is \"L8_UTC_YYYYMMDD_hhmmss.tif\"\n",
    "        time_str = filename.split('_')[2]\n",
    "        time_obj = datetime.strptime(time_str, \"%Y%m%d\")\n",
    "        \n",
    "        # Load and preprocess the GeoDataFrame\n",
    "        gdf = preprocessing(file_path)\n",
    "        gdf['time'] = time_obj  # Append the datetime object as a new column\n",
    "        \n",
    "        # Append the processed GeoDataFrame to the list\n",
    "        gdfs.append(gdf)\n",
    "\n",
    "# Combine all GeoDataFrames into one\n",
    "combined_gdf = pd.concat(gdfs)\n",
    "\n",
    "shutil.rmtree(f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\temp_extract\")\n",
    "\n",
    "# Use the combined GeoDataFrame as needed\n",
    "print(combined_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spatial plot over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying plot for 2020-01-06\n",
      "Number of pixels in region of interest: 32\n",
      "Launching server at http://localhost:49404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<panel.io.server.Server at 0x22746da32b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import panel as pn\n",
    "\n",
    "# Create an interactive plot with filtering based on the GeoDataFrame\n",
    "def create_interactive_plot(combined_gdf):\n",
    "    # Create a list of unique dates sorted\n",
    "    unique_dates = combined_gdf['time'].dt.strftime('%Y-%m-%d').sort_values().unique()\n",
    "    # print(f\"Unique Dates: {unique_dates}\")\n",
    "\n",
    "    date_index_map = {i + 1: date for i, date in enumerate(unique_dates)}\n",
    "\n",
    "    # Setup an integer slider to select time periods\n",
    "    time_slider = pn.widgets.IntSlider(name='Select Time', start=1, end=len(unique_dates), value=1, step=1)\n",
    "\n",
    "    @pn.depends(time_slider.param.value_throttled)\n",
    "    def dynamic_map(value):\n",
    "        selected_date = date_index_map[value]\n",
    "        selected_datetime = pd.to_datetime(selected_date).date()\n",
    "        \n",
    "        # Filter data for the selected time\n",
    "        filtered_data = combined_gdf[combined_gdf['time'].dt.date == selected_datetime]\n",
    "        print(f\"Displaying plot for \" + str(selected_date))\n",
    "        \n",
    "        # Call plot_spatial_map for the selected time period\n",
    "        return plot_spatial_map(filtered_data)\n",
    "\n",
    "    layout = pn.Column(\n",
    "        \"<br>\\nInteractive Land Surface Temperature Map\",\n",
    "        time_slider,\n",
    "        dynamic_map\n",
    "    )\n",
    "\n",
    "    return layout\n",
    "\n",
    "layout = create_interactive_plot(combined_gdf)\n",
    "# layout.servable()\n",
    "pn.serve(layout, show=False, start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to C:\\LocalOneDrive\\Documents\\Desktop\\MTI\\UHI-Project\\MSE-ES-UHI\\Data\\FilteredData\\BukitPurmei\\BukitPurmei_Filtered_2019.csv\n"
     ]
    }
   ],
   "source": [
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# from shapely.geometry import box\n",
    "\n",
    "# def filter_and_save_data(year_gdf, polygons, output_file):\n",
    "#     # Ensure polygons are in EPSG:3857\n",
    "#     polygon_gdf = gpd.GeoDataFrame({'geometry': list(polygons.values())}, crs='epsg:4326')\n",
    "#     polygon_gdf = polygon_gdf.to_crs('epsg:3857')\n",
    "\n",
    "#     # Initialize an empty DataFrame to store all filtered data\n",
    "#     all_filtered_data = gpd.GeoDataFrame()\n",
    "\n",
    "#     for date in year_gdf['time'].dt.strftime('%Y-%m-%d').sort_values().unique():\n",
    "#         # Filter data for the specific date\n",
    "#         date_data = year_gdf[year_gdf['time'].dt.strftime('%Y-%m-%d') == date]\n",
    "\n",
    "#         # Convert CRS to EPSG:3857 and create 30m x 30m boxes around each point\n",
    "#         date_data = date_data.to_crs('epsg:3857')\n",
    "#         date_data['geometry'] = date_data['geometry'].apply(\n",
    "#             lambda x: box(x.x - 15, x.y - 15, x.x + 15, x.y + 15))\n",
    "\n",
    "#         # Filter points that intersect any polygon\n",
    "#         date_data['intersects'] = date_data['geometry'].apply(\n",
    "#             lambda geom: any(geom.intersects(poly) for poly in polygon_gdf['geometry']))\n",
    "#         filtered_data = date_data[date_data['intersects']].copy()\n",
    "\n",
    "#         # Append the filtered data of this date to the all_filtered_data DataFrame\n",
    "#         all_filtered_data = pd.concat([all_filtered_data, filtered_data], ignore_index=True)\n",
    "\n",
    "#     # Drop the 'geometry' column as it cannot be saved directly in CSV format\n",
    "#     all_filtered_data.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "#     # Save the aggregated filtered data to a CSV file\n",
    "#     all_filtered_data.to_csv(output_file, index=False)\n",
    "#     print(f\"Data successfully exported to {output_file}\")\n",
    "\n",
    "# combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])  # Ensure 'time' is a datetime object\n",
    "# output_path = 'C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\FilteredData\\\\BukitPurmei\\\\BukitPurmei_Filtered_2019.csv'\n",
    "# filter_and_save_data(combined_gdf, polygons, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Longitude  Latitude    SR_B1    SR_B2    SR_B3    SR_B4    SR_B5  \\\n",
      "0   103.826035  1.275400   8174.0   8571.0  10142.0   9123.0  13348.0   \n",
      "1   103.826304  1.275400   7749.0   8301.0  10300.0   9246.0  13388.0   \n",
      "2   103.826574  1.275400   6842.0   7768.0   9908.0   8798.0  12583.0   \n",
      "3   103.826843  1.275400   6807.0   7961.0  10534.0   9713.0  13974.0   \n",
      "4   103.825496  1.275128   9156.0   9874.0  11983.0  11517.0  16463.0   \n",
      "5   103.825765  1.275128  10502.0  11065.0  13105.0  13540.0  17050.0   \n",
      "6   103.826035  1.275128   9550.0  10899.0  13272.0  12828.0  17480.0   \n",
      "7   103.826304  1.275128   6931.0   8435.0  11773.0  11180.0  15014.0   \n",
      "8   103.826574  1.275129   6617.0   6831.0  10033.0   9308.0  13623.0   \n",
      "9   103.825496  1.274857   9585.0  10752.0  12571.0  12402.0  17673.0   \n",
      "10  103.825765  1.274857   8620.0  10140.0  12882.0  12468.0  18143.0   \n",
      "11  103.826035  1.274857   7399.0   8354.0  11745.0  11578.0  15699.0   \n",
      "12  103.826304  1.274857   6928.0   7160.0  10353.0  10445.0  14723.0   \n",
      "13  103.826574  1.274857   6657.0   7026.0   9520.0   9385.0  14266.0   \n",
      "14  103.825496  1.274585   9627.0  10820.0  12752.0  12547.0  16904.0   \n",
      "15  103.825765  1.274585   9363.0   9477.0  12136.0  12927.0  15971.0   \n",
      "16  103.826035  1.274586   8274.0   9996.0  13115.0  12458.0  17106.0   \n",
      "17  103.826305  1.274586   7513.0   8849.0  12051.0  11412.0  17385.0   \n",
      "18  103.826574  1.274586   6078.0   6016.0   9323.0   8984.0  13597.0   \n",
      "19  103.825496  1.274314   8781.0   9385.0  11617.0  10633.0  15189.0   \n",
      "20  103.825765  1.274314   7436.0   9217.0  11634.0   9773.0  15922.0   \n",
      "21  103.826035  1.274314   6801.0   7762.0  10880.0  10808.0  15387.0   \n",
      "22  103.826305  1.274314   5902.0   6894.0  10000.0   9307.0  15551.0   \n",
      "\n",
      "      SR_B6    SR_B7  SR_QA_AEROSOL  ...  ST_ATRAN_Scaled  ST_CDIST_Scaled  \\\n",
      "0   11469.0  10019.0          224.0  ...           0.3423             0.18   \n",
      "1   11577.0   9898.0          224.0  ...           0.3423             0.15   \n",
      "2   11295.0  10145.0          224.0  ...           0.3423             0.12   \n",
      "3   12294.0  10610.0          224.0  ...           0.3423             0.09   \n",
      "4   14405.0  11501.0          224.0  ...           0.3423             0.24   \n",
      "5   16260.0  12728.0          224.0  ...           0.3423             0.21   \n",
      "6   16044.0  13195.0          224.0  ...           0.3423             0.18   \n",
      "7   14498.0  12586.0          224.0  ...           0.3423             0.15   \n",
      "8   12426.0  10844.0          224.0  ...           0.3423             0.12   \n",
      "9   15703.0  12538.0          224.0  ...           0.3423             0.24   \n",
      "10  16727.0  13731.0          224.0  ...           0.3423             0.21   \n",
      "11  15793.0  12653.0          192.0  ...           0.3423             0.18   \n",
      "12  14609.0  12092.0          224.0  ...           0.3423             0.15   \n",
      "13  13003.0  11583.0          224.0  ...           0.3423             0.12   \n",
      "14  14894.0  12323.0          224.0  ...           0.3423             0.24   \n",
      "15  15831.0  11982.0          224.0  ...           0.3423             0.21   \n",
      "16  15720.0  12621.0          224.0  ...           0.3423             0.18   \n",
      "17  15126.0  12535.0          224.0  ...           0.3423             0.15   \n",
      "18  12983.0  11220.0          224.0  ...           0.3423             0.12   \n",
      "19  13346.0  11534.0          224.0  ...           0.3423             0.24   \n",
      "20  12884.0  11567.0          224.0  ...           0.3423             0.21   \n",
      "21  14597.0  12551.0          224.0  ...           0.3423             0.18   \n",
      "22  13911.0  11841.0          224.0  ...           0.3423             0.15   \n",
      "\n",
      "    ST_DRAD_Scaled  ST_EMIS_Scaled  ST_EMSD_Scaled  ST_QA_Scaled  \\\n",
      "0             2.18          0.9597          0.0011          6.38   \n",
      "1             2.18          0.9597          0.0011          6.52   \n",
      "2             2.18          0.9597          0.0011          6.68   \n",
      "3             2.18          0.9597          0.0011          6.85   \n",
      "4             2.18          0.9656          0.0000          6.10   \n",
      "5             2.18          0.9656          0.0000          6.24   \n",
      "6             2.18          0.9597          0.0011          6.38   \n",
      "7             2.18          0.9597          0.0011          6.53   \n",
      "8             2.18          0.9597          0.0011          6.68   \n",
      "9             2.18          0.9642          0.0000          6.10   \n",
      "10            2.18          0.9642          0.0000          6.24   \n",
      "11            2.18          0.9697          0.0056          6.40   \n",
      "12            2.18          0.9697          0.0056          6.55   \n",
      "13            2.18          0.9697          0.0056          6.70   \n",
      "14            2.18          0.9642          0.0000          6.09   \n",
      "15            2.18          0.9642          0.0000          6.23   \n",
      "16            2.18          0.9697          0.0056          6.39   \n",
      "17            2.18          0.9697          0.0056          6.55   \n",
      "18            2.18          0.9697          0.0056          6.70   \n",
      "19            2.18          0.9642          0.0000          6.08   \n",
      "20            2.18          0.9642          0.0000          6.22   \n",
      "21            2.18          0.9697          0.0056          6.39   \n",
      "22            2.18          0.9697          0.0056          6.54   \n",
      "\n",
      "    ST_TRAD_Scaled  ST_URAD_Scaled       time        block  \n",
      "0         8.049001           5.112 2021-11-05  polygon_114  \n",
      "1         8.043000           5.112 2021-11-05  polygon_114  \n",
      "2         8.022000           5.112 2021-11-05  polygon_115  \n",
      "3         7.996000           5.112 2021-11-05  polygon_115  \n",
      "4         8.052000           5.112 2021-11-05  polygon_114  \n",
      "5         8.050000           5.112 2021-11-05  polygon_114  \n",
      "6         8.047000           5.112 2021-11-05  polygon_114  \n",
      "7         8.036000           5.112 2021-11-05  polygon_114  \n",
      "8         8.020000           5.112 2021-11-05  polygon_115  \n",
      "9         8.053000           5.112 2021-11-05  polygon_114  \n",
      "10        8.051001           5.112 2021-11-05  polygon_114  \n",
      "11        8.045000           5.112 2021-11-05  polygon_114  \n",
      "12        8.035001           5.112 2021-11-05  polygon_112  \n",
      "13        8.021000           5.112 2021-11-05  polygon_113  \n",
      "14        8.056001           5.112 2021-11-05  polygon_112  \n",
      "15        8.057000           5.112 2021-11-05  polygon_112  \n",
      "16        8.050000           5.112 2021-11-05  polygon_112  \n",
      "17        8.037001           5.112 2021-11-05  polygon_112  \n",
      "18        8.023001           5.112 2021-11-05  polygon_113  \n",
      "19        8.069000           5.112 2021-11-05  polygon_112  \n",
      "20        8.074000           5.112 2021-11-05  polygon_112  \n",
      "21        8.061001           5.112 2021-11-05  polygon_112  \n",
      "22        8.040000           5.112 2021-11-05  polygon_112  \n",
      "\n",
      "[23 rows x 39 columns]\n",
      "Data successfully exported to C:\\LocalOneDrive\\Documents\\Desktop\\MTI\\UHI-Project\\MSE-ES-UHI\\Data\\FilteredData\\BukitPurmei\\Landsat9\\BukitPurmei_Filtered_2021_Blocks.csv\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "\n",
    "def filter_and_save_data(year_gdf, polygons, output_file):\n",
    "    # Convert the polygons dictionary to a GeoDataFrame\n",
    "    polygon_gdf = gpd.GeoDataFrame({\n",
    "        'block': list(polygons.keys()),   # Keys from your dictionary\n",
    "        'geometry': list(polygons.values())\n",
    "    }, crs='epsg:4326')\n",
    "    polygon_gdf = polygon_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # Initialize an empty DataFrame to store all filtered data\n",
    "    all_filtered_data = gpd.GeoDataFrame()\n",
    "\n",
    "    for date in year_gdf['time'].dt.strftime('%Y-%m-%d').sort_values().unique():\n",
    "        # Filter data for the specific date\n",
    "        date_data = year_gdf[year_gdf['time'].dt.strftime('%Y-%m-%d') == date]\n",
    "\n",
    "        # Convert CRS to EPSG:3857 and create 30m x 30m boxes around each point\n",
    "        date_data = date_data.to_crs('epsg:3857')\n",
    "        date_data['geometry'] = date_data['geometry'].apply(\n",
    "            lambda x: box(x.x - 15, x.y - 15, x.x + 15, x.y + 15))\n",
    "\n",
    "        # Determine the block for each point by finding which polygon it intersects\n",
    "        def find_block(geom):\n",
    "            for idx, poly in polygon_gdf.iterrows():\n",
    "                if geom.intersects(poly['geometry']):\n",
    "                    return poly['block']\n",
    "            return None  # Return None or an appropriate value if no intersection is found\n",
    "\n",
    "        date_data['block'] = date_data['geometry'].apply(find_block)\n",
    "\n",
    "        # Filter to keep only data that intersects with a polygon\n",
    "        filtered_data = date_data[date_data['block'].notnull()].copy()\n",
    "\n",
    "        # Append the filtered data of this date to the all_filtered_data DataFrame\n",
    "        all_filtered_data = pd.concat([all_filtered_data, filtered_data], ignore_index=True)\n",
    "\n",
    "    # Drop the 'geometry' column as it cannot be saved directly in CSV format\n",
    "    all_filtered_data.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "    print(all_filtered_data)\n",
    "\n",
    "    # Save the aggregated filtered data to a CSV file\n",
    "    all_filtered_data.to_csv(output_file, index=False)\n",
    "    print(f\"Data successfully exported to {output_file}\")\n",
    "\n",
    "# Example usage, assuming combined_gdf and polygons are defined earlier\n",
    "combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])  # Ensure 'time' is a datetime object\n",
    "output_path = 'C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\FilteredData\\\\BukitPurmei\\\\Landsat9\\\\BukitPurmei_Filtered_2021_Blocks.csv'\n",
    "filter_and_save_data(combined_gdf, polygons, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codes to combine data from 2019 to 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files were successfully concatenated and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the base file path\n",
    "base_path = r\"C:\\LocalOneDrive\\Documents\\Desktop\\MTI\\UHI-Project\\MSE-ES-UHI\\Data\\FilteredData\\BukitPurmei\\Landsat8\"\n",
    "\n",
    "# File names\n",
    "files = [\n",
    "    r\"BukitPurmei_Filtered_2019_Blocks.csv\",\n",
    "    r\"BukitPurmei_Filtered_2020_Blocks.csv\",\n",
    "    r\"BukitPurmei_Filtered_2021_Blocks.csv\"\n",
    "]\n",
    "\n",
    "# Read and concatenate the CSV files\n",
    "df_list = [pd.read_csv(f\"{base_path}\\\\{file_name}\") for file_name in files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(f\"{base_path}\\\\BukitPurmei_Filtered_2019_to_2021_Blocks.csv\", index=False)\n",
    "\n",
    "print(\"Files were successfully concatenated and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
