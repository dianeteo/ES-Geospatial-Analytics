{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Filtering by specific postal code for spatial map plotting (Tuas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting coordinates of polygons\n",
    "import geopandas as gpd\n",
    "\n",
    "geojson_path = \"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\SG_geojson\\\\SG.geojson\"\n",
    "geo_data = gpd.read_file(geojson_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYGON ((103.6253826 1.296722, 103.6259768 1.2964981, 103.6260502 1.2966928, 103.625456 1.2969166, 103.6253826 1.296722))\n",
      "POLYGON ((103.6252304 1.2963046, 103.6258405 1.2960748, 103.6259422 1.2963444, 103.6255728 1.2964835, 103.625546 1.2964123, 103.6253051 1.296503, 103.6252304 1.2963046))\n",
      "POLYGON ((103.6250215 1.2959266, 103.6257117 1.2956666, 103.625809 1.2959247, 103.6254171 1.2960723, 103.6253902 1.296001, 103.6253122 1.2960304, 103.625338 1.2960988, 103.6251177 1.2961818, 103.6250919 1.2961134, 103.6250215 1.2959266))\n",
      "POLYGON ((103.6249362 1.2954636, 103.625529 1.2952402, 103.6256035 1.2954378, 103.6250106 1.2956612, 103.6249362 1.2954636))\n"
     ]
    }
   ],
   "source": [
    "# blocks_of_interest = ['8', '10', '12', '14', '6', '16', '18', '58', '60', '62', '64', '66', '68', '70', '72', '74', '18']\n",
    "blocks_of_interest = ['8', '10', '12', '14']\n",
    "polygons = {}\n",
    "\n",
    "for block in blocks_of_interest:\n",
    "    matching_features = geo_data[(geo_data['addr_street'].str.contains(\"Tuas South Street 5\", na=False)) & \n",
    "                                (geo_data['addr_housenumber'] == f\"{block}\")]\n",
    "\n",
    "    if not matching_features.empty:\n",
    "        polygon = matching_features.iloc[0]['geometry']\n",
    "        polygons[f'polygon_{block}'] = polygon\n",
    "        print(polygons[f'polygon_{block}'])\n",
    "    else:\n",
    "        print(\"No matching features found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postal Codes in Tuas for specified blocks: ['637793' '637791' '637792' '637794']\n"
     ]
    }
   ],
   "source": [
    "# Getting postal code\n",
    "postalcode_geojson_path = \"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\ADDRPT.geojson\"\n",
    "\n",
    "# Load GeoJSON data into a GeoDataFrame\n",
    "postalcode_gdf = gpd.read_file(postalcode_geojson_path)\n",
    "\n",
    "# print(postalcode_gdf.columns)\n",
    "\n",
    "def extract_postal_codes(geojson_path, road_name_keyword, blocks_of_interest):\n",
    "    # Filter for road name containing the specified keyword and block numbers of interest\n",
    "    filtered_gdf = postalcode_gdf[\n",
    "        (postalcode_gdf['ROAD_NAME'].str.contains(road_name_keyword, case=False, na=False)) &\n",
    "        (postalcode_gdf['HOUSE_BLK_NO'].isin(blocks_of_interest))\n",
    "    ]\n",
    "\n",
    "    # Extract the postal codes\n",
    "    postal_codes = filtered_gdf['POSTAL_CODE'].dropna().unique()\n",
    "\n",
    "    return postal_codes\n",
    "\n",
    "# Extract postal codes\n",
    "postal_codes = extract_postal_codes(geojson_path, \"TUAS SOUTH STREET 5\", blocks_of_interest)\n",
    "\n",
    "# Print the results\n",
    "print(\"Postal Codes in Tuas for specified blocks:\", postal_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(postal_codes) == len(polygons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for postal code 637793: Longitude 103.62544947276622, Latitude 1.295906479085173\n",
      "Coordinates for postal code 637791: Longitude 103.62577104052976, Latitude 1.2967118335708088\n",
      "Coordinates for postal code 637792: Longitude 103.62561996344215, Latitude 1.296299212800991\n",
      "Coordinates for postal code 637794: Longitude 103.62531133658884, Latitude 1.2954706996532268\n",
      "\n",
      "Central coordinates:\n",
      "Longitude: 103.62553795333174, Latitude: 1.2960970562775498\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# Define a dictionary to store coordinates\n",
    "coordinates_dict = {}\n",
    "\n",
    "# Function to get coordinates by postal code\n",
    "def get_coordinates_by_postal_code(postal_code):\n",
    "    postal_data = postalcode_gdf[postalcode_gdf['POSTAL_CODE'] == postal_code]\n",
    "    if not postal_data.empty:\n",
    "        # Extract the coordinates of the first matching entry\n",
    "        longitude = postal_data.geometry.x.values[0]\n",
    "        latitude = postal_data.geometry.y.values[0]\n",
    "        return longitude, latitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Fetch coordinates for each postal code and store them\n",
    "for postal_code in postal_codes:\n",
    "    coordinates_dict[postal_code] = get_coordinates_by_postal_code(postal_code)\n",
    "\n",
    "# Extract coordinates and calculate the central point\n",
    "longitudes = []\n",
    "latitudes = []\n",
    "\n",
    "for postal_code, coords in coordinates_dict.items():\n",
    "    if coords[0] is not None and coords[1] is not None:\n",
    "        longitudes.append(coords[0])\n",
    "        latitudes.append(coords[1])\n",
    "        print(f'Coordinates for postal code {postal_code}: Longitude {coords[0]}, Latitude {coords[1]}')\n",
    "\n",
    "if longitudes and latitudes:\n",
    "    avg_longitude = np.mean(longitudes)\n",
    "    avg_latitude = np.mean(latitudes)\n",
    "    print(f'\\nCentral coordinates:')\n",
    "    print(f'Longitude: {avg_longitude}, Latitude: {avg_latitude}')\n",
    "else:\n",
    "    print('Coordinates for some or all postal codes not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting x and y to coordinates for latitude/longitude\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import Point\n",
    "\n",
    "global filtered_df\n",
    "\n",
    "def preprocessing(file_path):   \n",
    "    global filtered_df\n",
    "    \n",
    "    # Open your GeoTIFF file\n",
    "    with rasterio.open(file_path) as src:\n",
    "        array = src.read()\n",
    "        transform = src.transform\n",
    "        src_crs = src.crs  # Source CRS\n",
    "        # dest_crs = 'EPSG:4326'  # WGS 84\n",
    "\n",
    "        # Create a transformer object to convert from src_crs to dest_crs\n",
    "        transformer = Transformer.from_crs(src_crs, 'EPSG:4326', always_xy=True)\n",
    "\n",
    "        # Get arrays of column and row indices\n",
    "        cols, rows = np.meshgrid(np.arange(array.shape[2]), np.arange(array.shape[1]))\n",
    "        \n",
    "        # Convert meshgrid arrays to coordinate arrays using rasterio's method, which are 2D\n",
    "        xs, ys = rasterio.transform.xy(transform, rows, cols, offset='center')\n",
    "        \n",
    "        # Flatten the coordinate arrays to pass to transform function\n",
    "        lon, lat = transformer.transform(np.array(xs).flatten(), np.array(ys).flatten())\n",
    "\n",
    "        # Create DataFrame and convert to GeoDataFrame\n",
    "        df = pd.DataFrame({'Longitude': lon, 'Latitude': lat})\n",
    "        for i, band in enumerate(src.read(masked=True)):\n",
    "            df[src.descriptions[i]] = band.flatten()\n",
    "\n",
    "        # # Convert 'SR_QA_AEROSOL' to integer for bitwise operation\n",
    "        # df['SR_QA_AEROSOL'] = df['SR_QA_AEROSOL'].astype(int)\n",
    "\n",
    "        # # Filter out pixels with valid aerosol retrieval and high aerosol level\n",
    "        # # Assuming 'SR_QA_AEROSOL' is the name of the QA aerosol band in the data\n",
    "        # valid_aerosol = (df['SR_QA_AEROSOL'] & 2) == 2  # Bit 1 must be set for valid retrieval\n",
    "        # high_aerosol = (df['SR_QA_AEROSOL'] & 192) == 192  # Bits 6-7 must be set to 11 for high aerosol\n",
    "        # filter_mask = valid_aerosol & high_aerosol\n",
    "        # df_filtered = df[-filter_mask]\n",
    "\n",
    "        df_filtered = df\n",
    "        \n",
    "        # Scale and offset specific bands\n",
    "        df_filtered['ST_B6_Celsius'] = df_filtered['ST_B6'] * 0.00341802 + 149 - 273.15\n",
    "        df_filtered = df_filtered[df_filtered['ST_B6_Celsius'] >= 20]  # Drop rows below 20 degrees Celsius\n",
    "        \n",
    "        bands_to_scale = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']\n",
    "        for band in bands_to_scale:\n",
    "            df_filtered[f\"{band}_Scaled\"] = df_filtered[band] * 2.75e-05 - 0.2\n",
    "\n",
    "        additional_scales = {\n",
    "            'ST_ATRAN': 0.0001, 'ST_CDIST': 0.01, 'ST_DRAD': 0.001, \n",
    "            'ST_EMIS': 0.0001, 'ST_EMSD': 0.0001, 'ST_QA': 0.01, \n",
    "            'ST_TRAD': 0.001, 'ST_URAD': 0.001\n",
    "        }\n",
    "\n",
    "        for band, scale in additional_scales.items():\n",
    "            df_filtered[f\"{band}_Scaled\"] = df_filtered[band] * scale\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(df_filtered, geometry=gpd.points_from_xy(df_filtered.Longitude, df_filtered.Latitude))\n",
    "        gdf.set_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "        print(\"Total number of valid pixels: \" + str(len(gdf)))\n",
    "        print(df[['Latitude', 'Longitude']].head())\n",
    "\n",
    "        gdf = gdf.to_crs('EPSG:3857')\n",
    "\n",
    "        transformer_2 = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "\n",
    "        avg_longitude_3857, avg_latitude_3857 = transformer_2.transform(avg_longitude, avg_latitude)\n",
    "\n",
    "        # Define your point of interest and buffer distance in meters\n",
    "        poi = Point(avg_longitude_3857, avg_latitude_3857)\n",
    "        desired_radius = 1000\n",
    "        buffer = poi.buffer(desired_radius)  # Convert meters to degrees approximately\n",
    "\n",
    "        # Filter points within the buffer\n",
    "        filtered_gdf = gdf[gdf.geometry.within(buffer)]\n",
    "\n",
    "        # Save or process your filtered data\n",
    "        print(f\"\\nNumber of points within {desired_radius}m radius: {len(filtered_gdf)}\")\n",
    "        #print(filtered_gdf['ST_B10_Celsius'].head())\n",
    "\n",
    "        filtered_gdf = filtered_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "    return filtered_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering 30m x 30m pixels based on region of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using EPSG:3857 allows you to blow up the pixels in metres because the coordinate representation is in metres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon, box\n",
    "import panel as pn\n",
    "from bokeh.palettes import Inferno256\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "logging.getLogger('bokeh').setLevel(logging.ERROR)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "global within_polygon_gdf\n",
    "\n",
    "def plot_spatial_map(filtered_gdf): \n",
    "    global within_polygon_gdf\n",
    "    \n",
    "    filtered_gdf = filtered_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # print(filtered_gdf['geometry'])\n",
    "\n",
    "    # Create pixels as 30m x 30m boxes around each point\n",
    "    # Assuming each point is at the center of the pixel\n",
    "    half_width = 15  # half the width of the pixel in meters since the EPSG:3857 coordinate system is in metres\n",
    "    filtered_gdf['geometry'] = filtered_gdf['geometry'].apply(lambda x: box(x.x - half_width, x.y - half_width, x.x + half_width, x.y + half_width))\n",
    "\n",
    "    #print(filtered_gdf['geometry'])\n",
    "\n",
    "    # Create a GeoDataFrame from all polygons and convert CRS to match\n",
    "    polygon_gdf = gpd.GeoDataFrame({'geometry': list(polygons.values())}, crs='epsg:4326')\n",
    "    polygon_gdf_3857 = polygon_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # Filter points that intersect any polygon\n",
    "    def intersects_any_polygon(point):\n",
    "        return any(point.intersects(poly) for poly in polygon_gdf['geometry'])\n",
    "    \n",
    "    filtered_gdf['intersects'] = filtered_gdf['geometry'].apply(intersects_any_polygon)\n",
    "\n",
    "    # Check intersection with any polygon\n",
    "    within_polygon_gdf = filtered_gdf[filtered_gdf['intersects']].copy()\n",
    "\n",
    "    # print(polygon_gdf_3857['geometry'])\n",
    "\n",
    "    # Filter points that intersect any polygon\n",
    "    filtered_gdf['intersects'] = filtered_gdf['geometry'].apply(\n",
    "        lambda geom: any(geom.intersects(poly) for poly in polygon_gdf_3857['geometry']))\n",
    "    within_polygon_gdf = filtered_gdf[filtered_gdf['intersects']].copy()\n",
    "\n",
    "    # print(within_polygon_gdf['SR_QA_AEROSOL'].unique())\n",
    "\n",
    "    print(\"Number of pixels in region of interest: \" + str(len(within_polygon_gdf)))\n",
    "\n",
    "    # Print or use the filtered GeoDataFrame as needed\n",
    "    # print(\"\\nNumber of points within the region of interest: \" + str(len(within_polygon_gdf)))\n",
    "\n",
    "    # # Print the centroids of the intersected pixels\n",
    "    # for index, row in within_polygon_gdf.iterrows():\n",
    "    #     centroid = row['geometry'].centroid\n",
    "    #     print(f\"Longitude: {centroid.x}, Latitude: {centroid.y}\")\n",
    "\n",
    "    # Define a function to select a subset of the color palette\n",
    "    def select_colors(palette, n):\n",
    "        return [palette[int(i)] for i in np.linspace(0, len(palette)-1, n)]\n",
    "\n",
    "    # Create a custom color scale using a continuous palette\n",
    "    custom_palette = select_colors(Inferno256, 256)  # More colors for smoother transitions\n",
    "\n",
    "    # Create the heatmap using the centroid points of intersected pixels\n",
    "    heatmap = within_polygon_gdf.hvplot.points('Longitude', 'Latitude', geo=True, c='ST_B6_Celsius', cmap=custom_palette, size=5, tiles='OSM', frame_width=700, frame_height=500, colorbar=True, clim=(20, 40))\n",
    "\n",
    "    # Plot square polygons with the same color mapping as the points\n",
    "    squares_plot = within_polygon_gdf.hvplot.polygons('geometry', c='ST_B6_Celsius', cmap=custom_palette, alpha=0.5, colorbar=True, clim=(20, 40))\n",
    "\n",
    "    # Plot the polygon with visible settings\n",
    "    polygon_plot = polygon_gdf.hvplot(geo=True, color='red', line_width=3, alpha=0.7)\n",
    "\n",
    "    # Overlay the polygon onto the heatmap\n",
    "    overlay_map = polygon_plot * heatmap * squares_plot\n",
    "\n",
    "    # Set up Panel to display the plot\n",
    "    # pane = pn.panel(overlay_map)\n",
    "\n",
    "    # pane.show()\n",
    "    # pane.save(f'C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\MSE-ES-UHI\\\\2_landsat\\\\Heatmaps\\\\{postal_code_112}_{satellite_image}_LST_Filtered.html', embed=True)\n",
    "\n",
    "    return overlay_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting LST over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining GDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing: L7_UTC_20200403_025238.tif\n",
      "Total number of valid pixels: 10985\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 0\n",
      "Currently processing: L7_UTC_20200419_025142.tif\n",
      "Total number of valid pixels: 1257048\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 463\n",
      "Currently processing: L7_UTC_20200505_025045.tif\n",
      "Total number of valid pixels: 1295831\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 2521\n",
      "Currently processing: L7_UTC_20200521_024946.tif\n",
      "Total number of valid pixels: 1103794\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 5\n",
      "Currently processing: L7_UTC_20200724_024557.tif\n",
      "Total number of valid pixels: 59124\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 4\n",
      "Currently processing: L7_UTC_20200809_024457.tif\n",
      "Total number of valid pixels: 443458\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 2549\n",
      "Currently processing: L7_UTC_20200825_024355.tif\n",
      "Total number of valid pixels: 2\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 0\n",
      "Currently processing: L7_UTC_20200910_024252.tif\n",
      "Total number of valid pixels: 7\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 0\n",
      "Currently processing: L7_UTC_20201028_023945.tif\n",
      "Total number of valid pixels: 0\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 0\n",
      "Currently processing: L7_UTC_20201113_023840.tif\n",
      "Total number of valid pixels: 1326152\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 2266\n",
      "Currently processing: L7_UTC_20201215_023620.tif\n",
      "Total number of valid pixels: 897523\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 1000m radius: 2666\n",
      "          Longitude  Latitude    SR_B1    SR_B2    SR_B3    SR_B4    SR_B5  \\\n",
      "1151336  103.634068  1.294841  16868.0  17045.0  17014.0  18222.0  16847.0   \n",
      "1153110  103.631911  1.294568   9975.0  10296.0  10132.0  11970.0  11030.0   \n",
      "1153111  103.632181  1.294568  11467.0  11512.0  11502.0  12760.0  12215.0   \n",
      "1153112  103.632450  1.294569  11021.0  11351.0  11358.0  12129.0  11851.0   \n",
      "1153113  103.632720  1.294569  11170.0  11271.0  10927.0  12129.0  11213.0   \n",
      "...             ...       ...      ...      ...      ...      ...      ...   \n",
      "1194085  103.628949  1.288326   8609.0  10073.0   9012.0  23159.0  14657.0   \n",
      "1194086  103.629218  1.288326   8525.0  10073.0   8931.0  22827.0  14558.0   \n",
      "1194087  103.629488  1.288326   8609.0  10163.0   9091.0  21659.0  14558.0   \n",
      "1194088  103.629758  1.288326   8692.0  10163.0   9091.0  21826.0  14361.0   \n",
      "1195869  103.629488  1.288055   8609.0   9895.0   9250.0  20990.0  14262.0   \n",
      "\n",
      "           SR_B7  SR_ATMOS_OPACITY  SR_CLOUD_QA  ...  ST_ATRAN_Scaled  \\\n",
      "1151336  14813.0             354.0          2.0  ...           0.4233   \n",
      "1153110   9994.0             356.0          8.0  ...           0.4233   \n",
      "1153111  11731.0             356.0          8.0  ...           0.4233   \n",
      "1153112  11442.0             356.0          8.0  ...           0.4233   \n",
      "1153113  10283.0             356.0          8.0  ...           0.4233   \n",
      "...          ...               ...          ...  ...              ...   \n",
      "1194085  10261.0             331.0          NaN  ...           0.3487   \n",
      "1194086  10366.0             331.0          NaN  ...           0.3487   \n",
      "1194087  10261.0             331.0          NaN  ...           0.3487   \n",
      "1194088  10366.0             331.0          NaN  ...           0.3487   \n",
      "1195869   9943.0             331.0          NaN  ...           0.3487   \n",
      "\n",
      "         ST_CDIST_Scaled  ST_DRAD_Scaled  ST_EMIS_Scaled  ST_EMSD_Scaled  \\\n",
      "1151336             0.00           1.971          0.9061          0.0310   \n",
      "1153110             0.00           1.971          0.9670          0.0082   \n",
      "1153111             0.00           1.971          0.9527          0.0221   \n",
      "1153112             0.00           1.971          0.9509          0.0221   \n",
      "1153113             0.00           1.971          0.9528          0.0221   \n",
      "...                  ...             ...             ...             ...   \n",
      "1194085             3.66           2.087          0.9559          0.0081   \n",
      "1194086             3.66           2.087          0.9525          0.0107   \n",
      "1194087             3.67           2.087          0.9525          0.0107   \n",
      "1194088             3.67           2.087          0.9525          0.0107   \n",
      "1195869             3.64           2.087          0.9525          0.0107   \n",
      "\n",
      "         ST_QA_Scaled  ST_TRAD_Scaled  ST_URAD_Scaled  \\\n",
      "1151336          7.89        7.962000           4.596   \n",
      "1153110          7.68        8.111000           4.596   \n",
      "1153111          7.72        8.148001           4.596   \n",
      "1153112          7.69        8.185000           4.596   \n",
      "1153113          7.69        8.185000           4.596   \n",
      "...               ...             ...             ...   \n",
      "1194085          3.62        8.185000           4.986   \n",
      "1194086          3.64        8.185000           4.986   \n",
      "1194087          3.66        8.148001           4.986   \n",
      "1194088          3.64        8.185000           4.986   \n",
      "1195869          3.64        8.185000           4.986   \n",
      "\n",
      "                          geometry       time  \n",
      "1151336  POINT (103.63407 1.29484) 2020-04-19  \n",
      "1153110  POINT (103.63191 1.29457) 2020-04-19  \n",
      "1153111  POINT (103.63218 1.29457) 2020-04-19  \n",
      "1153112  POINT (103.63245 1.29457) 2020-04-19  \n",
      "1153113  POINT (103.63272 1.29457) 2020-04-19  \n",
      "...                            ...        ...  \n",
      "1194085  POINT (103.62895 1.28833) 2020-12-15  \n",
      "1194086  POINT (103.62922 1.28833) 2020-12-15  \n",
      "1194087  POINT (103.62949 1.28833) 2020-12-15  \n",
      "1194088  POINT (103.62976 1.28833) 2020-12-15  \n",
      "1195869  POINT (103.62949 1.28805) 2020-12-15  \n",
      "\n",
      "[10474 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "# Required data is from 2022 - 2024\n",
    "year = \"2020\"\n",
    "\n",
    "# Suppress warnings\n",
    "logging.getLogger('bokeh').setLevel(logging.ERROR)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Specify the zip file and temporary directory for extraction\n",
    "zip_file_path = f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\Landsat7\\\\{year}.zip\"\n",
    "temp_dir = f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\temp_extract\"\n",
    "\n",
    "# Create a temporary directory if it doesn't exist\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Extract the .tif files from the zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "# Initialize an empty list to hold all the GeoDataFrames\n",
    "gdfs = []\n",
    "\n",
    "# Walk through the temporary directory and process each .tif file\n",
    "for filename in os.listdir(f\"{temp_dir}\\\\{year}\"):\n",
    "    if filename.endswith(\".tif\"):\n",
    "        print(\"Currently processing: \" + filename)\n",
    "        file_path = os.path.join(f\"{temp_dir}\\\\{year}\", filename)\n",
    "        \n",
    "        # Extract the time period from the filename\n",
    "        # Assuming filename format is \"L8_UTC_YYYYMMDD_hhmmss.tif\"\n",
    "        time_str = filename.split('_')[2]\n",
    "        time_obj = datetime.strptime(time_str, \"%Y%m%d\")\n",
    "        \n",
    "        # Load and preprocess the GeoDataFrame\n",
    "        gdf = preprocessing(file_path)\n",
    "        gdf['time'] = time_obj  # Append the datetime object as a new column\n",
    "        \n",
    "        # Append the processed GeoDataFrame to the list\n",
    "        gdfs.append(gdf)\n",
    "\n",
    "# Combine all GeoDataFrames into one\n",
    "combined_gdf = pd.concat(gdfs)\n",
    "\n",
    "shutil.rmtree(f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\temp_extract\")\n",
    "\n",
    "# Use the combined GeoDataFrame as needed\n",
    "print(combined_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import hvplot.pandas\n",
    "# import holoviews as hv\n",
    "# from bokeh.palettes import Turbo256  # Import a predefined Bokeh palette\n",
    "\n",
    "# # Assuming 'gdf' is your preloaded GeoDataFrame\n",
    "# combined_gdf = combined_gdf.to_crs(epsg=3857)  # Convert to Web Mercator for better mapping support\n",
    "\n",
    "# # Define a function to select a subset of the color palette\n",
    "# def select_colors(palette, n):\n",
    "#     return [palette[int(i)] for i in np.linspace(0, len(palette)-1, n)]\n",
    "\n",
    "# # Create a custom color scale using a continuous palette\n",
    "# custom_palette = select_colors(Turbo256, 256)  # More colors for smoother transitions\n",
    "\n",
    "# # Create the heatmap\n",
    "# heatmap = combined_gdf.hvplot.points('Longitude', 'Latitude', geo=True, c='ST_B10_Celsius',\n",
    "#                             cmap=custom_palette, size=5,  # Smaller size for finer detail\n",
    "#                             tiles='OSM', frame_width=700, frame_height=500,\n",
    "#                             colorbar=True, clim=(20, 40))\n",
    "\n",
    "# #file_path = \"C:/LocalOneDrive/Documents/Desktop/MTI/UHI-Project/MSE-ES-UHI/MSE-ES-UHI/2_landsat/Heatmaps\"\n",
    "\n",
    "# # Set up Panel to display the plot\n",
    "# heatmap_panel = hv.save(heatmap, '270524_hvPlot_Land_Surface_Temperature_Map_gradient.html', backend='bokeh')\n",
    "\n",
    "# # Display the plot in the notebook\n",
    "# heatmap_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spatial plot over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying plot for 2020-04-19\n",
      "Number of pixels in region of interest: 0\n",
      "Launching server at http://localhost:50091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<panel.io.server.Server at 0x17a25223dc0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying plot for 2020-05-05\n",
      "Number of pixels in region of interest: 25\n"
     ]
    }
   ],
   "source": [
    "import panel as pn\n",
    "\n",
    "# Create an interactive plot with filtering based on the GeoDataFrame\n",
    "def create_interactive_plot(combined_gdf):\n",
    "    # Create a list of unique dates sorted\n",
    "    unique_dates = combined_gdf['time'].dt.strftime('%Y-%m-%d').sort_values().unique()\n",
    "    # print(f\"Unique Dates: {unique_dates}\")\n",
    "\n",
    "    date_index_map = {i + 1: date for i, date in enumerate(unique_dates)}\n",
    "\n",
    "    # Setup an integer slider to select time periods\n",
    "    time_slider = pn.widgets.IntSlider(name='Select Time', start=1, end=len(unique_dates), value=1, step=1)\n",
    "\n",
    "    @pn.depends(time_slider.param.value_throttled)\n",
    "    def dynamic_map(value):\n",
    "        selected_date = date_index_map[value]\n",
    "        selected_datetime = pd.to_datetime(selected_date).date()\n",
    "        \n",
    "        # Filter data for the selected time\n",
    "        filtered_data = combined_gdf[combined_gdf['time'].dt.date == selected_datetime]\n",
    "        print(f\"Displaying plot for \" + str(selected_date))\n",
    "        \n",
    "        # Call plot_spatial_map for the selected time period\n",
    "        return plot_spatial_map(filtered_data)\n",
    "\n",
    "    layout = pn.Column(\n",
    "        \"<br>\\nInteractive Land Surface Temperature Map\",\n",
    "        time_slider,\n",
    "        dynamic_map\n",
    "    )\n",
    "\n",
    "    return layout\n",
    "\n",
    "layout = create_interactive_plot(combined_gdf)\n",
    "# layout.servable()\n",
    "pn.serve(layout, show=False, start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to C:\\LocalOneDrive\\Documents\\Desktop\\MTI\\UHI-Project\\MSE-ES-UHI\\Data\\FilteredData\\Tuas\\Landsat7\\Tuas_Filtered_2020.csv\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "import warnings\n",
    "\n",
    "# Suppress specific FutureWarnings related to GeoPandas\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The `op` parameter is deprecated\")\n",
    "\n",
    "def filter_and_save_data(year_gdf, polygons, output_file):\n",
    "    # Ensure polygons are in EPSG:3857\n",
    "    polygon_gdf = gpd.GeoDataFrame({'geometry': list(polygons.values()), 'BLOCK_NO': list(polygons.keys())}, crs='epsg:4326')\n",
    "    polygon_gdf = polygon_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # Initialize an empty DataFrame to store all filtered data\n",
    "    all_filtered_data = gpd.GeoDataFrame()\n",
    "\n",
    "    for date in year_gdf['time'].dt.strftime('%Y-%m-%d').sort_values().unique():\n",
    "        # Filter data for the specific date\n",
    "        date_data = year_gdf[year_gdf['time'].dt.strftime('%Y-%m-%d') == date]\n",
    "\n",
    "        # Convert CRS to EPSG:3857 and create 30m x 30m boxes around each point\n",
    "        date_data = date_data.to_crs('epsg:3857')\n",
    "        date_data['geometry'] = date_data['geometry'].apply(\n",
    "            lambda x: box(x.x - 15, x.y - 15, x.x + 15, x.y + 15))\n",
    "\n",
    "        # Perform spatial join to determine intersecting polygons\n",
    "        joined_data = gpd.sjoin(date_data, polygon_gdf, how='inner', op='intersects')\n",
    "\n",
    "        # Append the filtered data of this date to the all_filtered_data DataFrame\n",
    "        all_filtered_data = pd.concat([all_filtered_data, joined_data], ignore_index=True)\n",
    "\n",
    "    # Drop columns that are not needed and duplicates if necessary\n",
    "    all_filtered_data.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "    # Save the aggregated filtered data to a CSV file\n",
    "    all_filtered_data.to_csv(output_file, index=False)\n",
    "    print(f\"Data successfully exported to {output_file}\")\n",
    "\n",
    "# Assume combined_gdf is already loaded with time converted to datetime\n",
    "output_path = 'C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\FilteredData\\\\Tuas\\\\Landsat7\\\\Tuas_Filtered_2020.csv'\n",
    "filter_and_save_data(combined_gdf, polygons, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Codes to combine .csv files for 2022 - 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files were successfully concatenated and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the base file path\n",
    "base_path = r\"C:\\LocalOneDrive\\Documents\\Desktop\\MTI\\UHI-Project\\MSE-ES-UHI\\Data\\FilteredData\\Tuas\\Landsat7\"\n",
    "\n",
    "# File names\n",
    "files = [\n",
    "    r\"Tuas_Filtered_2017.csv\",\n",
    "    r\"Tuas_Filtered_2018.csv\",\n",
    "    r\"Tuas_Filtered_2019.csv\", \n",
    "    r\"Tuas_Filtered_2020.csv\"\n",
    "]\n",
    "\n",
    "# Read and concatenate the CSV files\n",
    "df_list = [pd.read_csv(f\"{base_path}\\\\{file_name}\") for file_name in files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(f\"{base_path}\\\\Tuas_Filtered_2017_to_2020.csv\", index=False)\n",
    "\n",
    "print(\"Files were successfully concatenated and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
