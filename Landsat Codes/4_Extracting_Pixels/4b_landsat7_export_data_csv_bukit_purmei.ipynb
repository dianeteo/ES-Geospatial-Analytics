{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import process\n",
    "import logging\n",
    "import warnings\n",
    "import shutil\n",
    "import hvplot.pandas\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "import panel as pn\n",
    "from bokeh.palettes import Inferno256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Filtering by specific postal code for spatial map plotting (Bukit Purmei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for postal code 090112: Longitude 103.82593292805574, Latitude 1.2745285256209595\n",
      "Coordinates for postal code 090114: Longitude 103.82588719010951, Latitude 1.2750718182249274\n",
      "Coordinates for postal code 090113: Longitude 103.82693226761857, Latitude 1.2747701258018154\n",
      "Coordinates for postal code 090115: Longitude 103.82695018030107, Latitude 1.2753520132629723\n"
     ]
    }
   ],
   "source": [
    "# Exporting data for blocks of interest and control blocks\n",
    "geojson_path = \"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\ADDRPT.geojson\"\n",
    "postal_code_112 = \"090112\"\n",
    "postal_code_114 = \"090114\"\n",
    "postal_code_113 = \"090113\"\n",
    "postal_code_115 = \"090115\"\n",
    "\n",
    "# Load the GeoJSON file\n",
    "gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Function to retrieve coordinates by postal code\n",
    "def get_coordinates_by_postal_code(postal_code):\n",
    "    # Filter GeoDataFrame for the given postal code\n",
    "    filtered_gdf = gdf[gdf['POSTAL_CODE'] == postal_code]\n",
    "    if not filtered_gdf.empty:\n",
    "        # Extract coordinates\n",
    "        point = filtered_gdf.iloc[0].geometry\n",
    "        return point.x, point.y\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "longitude_112, latitude_112 = get_coordinates_by_postal_code(postal_code_112)\n",
    "longitude_114, latitude_114 = get_coordinates_by_postal_code(postal_code_114)\n",
    "longitude_113, latitude_113 = get_coordinates_by_postal_code(postal_code_113)\n",
    "longitude_115, latitude_115 = get_coordinates_by_postal_code(postal_code_115)\n",
    "\n",
    "if longitude_112 and latitude_112 and longitude_114 and latitude_114:\n",
    "    print(f'Coordinates for postal code {postal_code_112}: Longitude {longitude_112}, Latitude {latitude_112}')\n",
    "    print(f'Coordinates for postal code {postal_code_114}: Longitude {longitude_114}, Latitude {latitude_114}')\n",
    "    print(f'Coordinates for postal code {postal_code_113}: Longitude {longitude_113}, Latitude {latitude_113}')\n",
    "    print(f'Coordinates for postal code {postal_code_115}: Longitude {longitude_115}, Latitude {latitude_115}')\n",
    "else:\n",
    "    print('Postal code not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central coordinates:\n",
      "Longitude: 103.82642564152123, Latitude: 1.2749306207276687\n"
     ]
    }
   ],
   "source": [
    "# Finding the central postal code for Blocks 112, 114, 113 and 115\n",
    "if longitude_112 and latitude_112 and longitude_114 and latitude_114:\n",
    "    # Calculate the average coordinates\n",
    "    avg_longitude = (longitude_112 + longitude_114 + longitude_113 + longitude_115) / 4\n",
    "    avg_latitude = (latitude_112 + latitude_114 + latitude_113 + latitude_115) / 4\n",
    "    print(f'Central coordinates:')\n",
    "    print(f'Longitude: {avg_longitude}, Latitude: {avg_latitude}')\n",
    "else:\n",
    "    print('Postal code not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting x and y to coordinates for latitude/longitude\n",
    "\n",
    "global filtered_df\n",
    "\n",
    "def preprocessing(file_path):   \n",
    "    global filtered_df\n",
    "    \n",
    "    # Open your GeoTIFF file\n",
    "    with rasterio.open(file_path) as src:\n",
    "        array = src.read()\n",
    "        transform = src.transform\n",
    "        src_crs = src.crs  # Source CRS\n",
    "        dest_crs = 'EPSG:4326'  # WGS 84\n",
    "\n",
    "        # Create a transformer object to convert from src_crs to dest_crs\n",
    "        transformer = Transformer.from_crs(src_crs, dest_crs, always_xy=True)\n",
    "\n",
    "        # Get arrays of column and row indices\n",
    "        cols, rows = np.meshgrid(np.arange(array.shape[2]), np.arange(array.shape[1]))\n",
    "        \n",
    "        # Convert meshgrid arrays to coordinate arrays using rasterio's method, which are 2D\n",
    "        xs, ys = rasterio.transform.xy(transform, rows, cols, offset='center')\n",
    "        \n",
    "        # Flatten the coordinate arrays to pass to transform function\n",
    "        lon, lat = transformer.transform(np.array(xs).flatten(), np.array(ys).flatten())\n",
    "\n",
    "        # Create DataFrame and convert to GeoDataFrame\n",
    "        df = pd.DataFrame({'Longitude': lon, 'Latitude': lat})\n",
    "        for i, band in enumerate(src.read(masked=True)):\n",
    "            df[src.descriptions[i]] = band.flatten()\n",
    "\n",
    "        # # Convert 'SR_QA_AEROSOL' to integer for bitwise operation\n",
    "        # df['SR_QA_AEROSOL'] = df['SR_QA_AEROSOL'].astype(int)\n",
    "\n",
    "        # # Filter out pixels with valid aerosol retrieval and high aerosol level\n",
    "        # # Assuming 'SR_QA_AEROSOL' is the name of the QA aerosol band in the data\n",
    "        # valid_aerosol = (df['SR_QA_AEROSOL'] & 2) == 2  # Bit 1 must be set for valid retrieval\n",
    "        # high_aerosol = (df['SR_QA_AEROSOL'] & 192) == 192  # Bits 6-7 must be set to 11 for high aerosol\n",
    "        # filter_mask = valid_aerosol & high_aerosol\n",
    "        # df_filtered = df[-filter_mask]\n",
    "\n",
    "        df_filtered = df\n",
    "        \n",
    "        # Scale and offset specific bands\n",
    "        df_filtered['ST_B6_Celsius'] = df_filtered['ST_B6'] * 0.00341802 + 149 - 273.15\n",
    "        df_filtered = df_filtered[df_filtered['ST_B6_Celsius'] >= 20]  # Drop rows below 20 degrees Celsius\n",
    "        \n",
    "        bands_to_scale = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']\n",
    "        for band in bands_to_scale:\n",
    "            df_filtered[f\"{band}_Scaled\"] = df_filtered[band] * 2.75e-05 - 0.2\n",
    "\n",
    "        additional_scales = {\n",
    "            'ST_ATRAN': 0.0001, 'ST_CDIST': 0.01, 'ST_DRAD': 0.001, \n",
    "            'ST_EMIS': 0.0001, 'ST_EMSD': 0.0001, 'ST_QA': 0.01, \n",
    "            'ST_TRAD': 0.001, 'ST_URAD': 0.001\n",
    "        }\n",
    "\n",
    "        for band, scale in additional_scales.items():\n",
    "            df_filtered[f\"{band}_Scaled\"] = df_filtered[band] * scale\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(df_filtered, geometry=gpd.points_from_xy(df_filtered.Longitude, df_filtered.Latitude))\n",
    "        gdf.set_crs('EPSG:4326', inplace=True)  # Ensure the CRS is set to WGS 84\n",
    "\n",
    "        print(\"Total number of valid pixels: \" + str(len(gdf)))\n",
    "        print(df[['Latitude', 'Longitude']].head())\n",
    "\n",
    "        # Define your point of interest and buffer distance in meters\n",
    "        poi = Point(avg_longitude, avg_latitude)\n",
    "        desired_radius = 200\n",
    "        buffer = poi.buffer(desired_radius / 111320)  # Convert meters to degrees approximately\n",
    "\n",
    "        # Filter points within the buffer\n",
    "        filtered_gdf = gdf[gdf.geometry.within(buffer)]\n",
    "\n",
    "        # Save or process your filtered data\n",
    "        print(f\"\\nNumber of points within {desired_radius}m radius: {len(filtered_gdf)}\")\n",
    "        #print(filtered_gdf['ST_B10_Celsius'].head())\n",
    "\n",
    "    return filtered_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining boundaries and plotting region of interest using GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['osm_id', 'osm_type', 'addr_street', 'building', 'name', 'access_roof',\n",
      "       'building_material', 'addr_housenumber', 'roof_material', 'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "geojson_path = \"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\SG_geojson\\\\SG.geojson\"\n",
    "\n",
    "geo_data = gpd.read_file(geojson_path)\n",
    "\n",
    "# Display the matching features\n",
    "print(geo_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYGON ((103.8254384 1.2743995, 103.825484 1.2743955, 103.8254826 1.2743647, 103.8257254 1.274362, 103.8257254 1.2744009, 103.8260325 1.2743915, 103.8260754 1.2744344, 103.8262618 1.274429, 103.8263745 1.2745524, 103.8263785 1.2747656, 103.8262082 1.2747669, 103.8262045 1.274549, 103.8257106 1.2745598, 103.8257079 1.2745108, 103.8254424 1.2745122, 103.8254384 1.2743995))\n",
      "POLYGON ((103.8254451 1.274657, 103.8255926 1.2746516, 103.8255886 1.2750116, 103.8256382 1.2750605, 103.8258664 1.2750513, 103.8258635 1.2749908, 103.8261706 1.2749835, 103.8261733 1.2750663, 103.82622 1.2750652, 103.8262177 1.2749582, 103.826303 1.2749563, 103.8263065 1.2751152, 103.8263989 1.2751132, 103.8264009 1.2752002, 103.8262434 1.2752037, 103.8262454 1.2752961, 103.8261471 1.2752983, 103.8261437 1.2751449, 103.8258625 1.2751533, 103.8258635 1.2751839, 103.8255618 1.2751933, 103.8254545 1.2750927, 103.8254451 1.274657))\n",
      "POLYGON ((103.8264643 1.2746563, 103.8268371 1.2746362, 103.8268412 1.2746898, 103.8270133 1.2746847, 103.827013 1.2747262, 103.8272475 1.274718, 103.8273682 1.2748239, 103.8273759 1.2750803, 103.8272046 1.275086, 103.8271993 1.2748183, 103.8268457 1.27483, 103.8268442 1.2748025, 103.8264737 1.2748159, 103.8264643 1.2746563))\n",
      "POLYGON ((103.8264791 1.2749426, 103.8266159 1.2749426, 103.8266239 1.2752985, 103.8266789 1.2753441, 103.826868 1.2753354, 103.8268653 1.2752871, 103.8270262 1.2752804, 103.8270262 1.2752308, 103.8272086 1.2752228, 103.8272096 1.2752788, 103.8272529 1.2752764, 103.8272489 1.2751624, 103.827336 1.2751598, 103.8273389 1.2753173, 103.8274248 1.2753141, 103.8274277 1.2754222, 103.8272797 1.2754266, 103.8272797 1.2755305, 103.8271832 1.2755338, 103.8271764 1.2753569, 103.8270665 1.2753622, 103.8270665 1.2754165, 103.8268895 1.2754239, 103.8268908 1.2754775, 103.8266011 1.2754815, 103.8264911 1.2753797, 103.8264791 1.2749426))\n"
     ]
    }
   ],
   "source": [
    "blocks_of_interest = ['112', '114', '113', '115']\n",
    "\n",
    "polygons = {}\n",
    "\n",
    "for block in blocks_of_interest:\n",
    "    matching_features = geo_data[(geo_data['addr_street'].str.contains(\"Bukit Purmei\", na=False)) & \n",
    "                                (geo_data['addr_housenumber'].str.contains(f\"{block}\", na=False))]\n",
    "\n",
    "    if not matching_features.empty:\n",
    "        polygon = matching_features.iloc[0]['geometry']\n",
    "        polygons[f'polygon_{block}'] = polygon\n",
    "        print(polygons[f'polygon_{block}'])\n",
    "    else:\n",
    "        print(\"No matching features found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering 30m x 30m pixels based on region of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using ESPG:3857 allows you to blow up the pixels in metres because the coordinate representation is in metres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.2/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='p1002'>\n",
       "  <div id=\"a23a7d61-8f9e-40df-b610-5b0c9930c048\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"f4da7b31-09b8-4411-81ff-a765e0f844ec\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"p1002\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"p1003\",\"attributes\":{\"plot_id\":\"p1002\",\"comm_id\":\"2859ff47137a44bea409e546dd040394\",\"client_comm_id\":\"015b81f237364965a71c16207d11991d\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"f4da7b31-09b8-4411-81ff-a765e0f844ec\",\"roots\":{\"p1002\":\"a23a7d61-8f9e-40df-b610-5b0c9930c048\"},\"root_ids\":[\"p1002\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "p1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "logging.getLogger('bokeh').setLevel(logging.ERROR)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "global within_polygon_gdf\n",
    "\n",
    "def plot_spatial_map(filtered_gdf): \n",
    "    global within_polygon_gdf\n",
    "    \n",
    "    filtered_gdf = filtered_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # print(filtered_gdf['geometry'])\n",
    "\n",
    "    # Create pixels as 30m x 30m boxes around each point\n",
    "    # Assuming each point is at the center of the pixel\n",
    "    half_width = 15  # half the width of the pixel in meters since the ESPG:3857 coordinate system is in metres\n",
    "    filtered_gdf['geometry'] = filtered_gdf['geometry'].apply(lambda x: box(x.x - half_width, x.y - half_width, x.x + half_width, x.y + half_width))\n",
    "\n",
    "    #print(filtered_gdf['geometry'])\n",
    "\n",
    "    # Create a GeoDataFrame from all polygons and convert CRS to match\n",
    "    polygon_gdf = gpd.GeoDataFrame({'geometry': list(polygons.values())}, crs='epsg:4326')\n",
    "    polygon_gdf_3857 = polygon_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # Filter points that intersect any polygon\n",
    "    def intersects_any_polygon(point):\n",
    "        return any(point.intersects(poly) for poly in polygon_gdf['geometry'])\n",
    "    \n",
    "    filtered_gdf['intersects'] = filtered_gdf['geometry'].apply(intersects_any_polygon)\n",
    "\n",
    "    # Check intersection with any polygon\n",
    "    within_polygon_gdf = filtered_gdf[filtered_gdf['intersects']].copy()\n",
    "\n",
    "    # print(polygon_gdf_3857['geometry'])\n",
    "\n",
    "    # Filter points that intersect any polygon\n",
    "    filtered_gdf['intersects'] = filtered_gdf['geometry'].apply(\n",
    "        lambda geom: any(geom.intersects(poly) for poly in polygon_gdf_3857['geometry']))\n",
    "    within_polygon_gdf = filtered_gdf[filtered_gdf['intersects']].copy()\n",
    "\n",
    "    print(\"Number of pixels in region of interest: \" + str(len(within_polygon_gdf)))\n",
    "\n",
    "    # Print or use the filtered GeoDataFrame as needed\n",
    "    # print(\"\\nNumber of points within the region of interest: \" + str(len(within_polygon_gdf)))\n",
    "\n",
    "    # # Print the centroids of the intersected pixels\n",
    "    # for index, row in within_polygon_gdf.iterrows():\n",
    "    #     centroid = row['geometry'].centroid\n",
    "    #     print(f\"Longitude: {centroid.x}, Latitude: {centroid.y}\")\n",
    "\n",
    "    # Define a function to select a subset of the color palette\n",
    "    def select_colors(palette, n):\n",
    "        return [palette[int(i)] for i in np.linspace(0, len(palette)-1, n)]\n",
    "\n",
    "    # Create a custom color scale using a continuous palette\n",
    "    custom_palette = select_colors(Inferno256, 256)  # More colors for smoother transitions\n",
    "\n",
    "    # Create the heatmap using the centroid points of intersected pixels\n",
    "    heatmap = within_polygon_gdf.hvplot.points('Longitude', 'Latitude', geo=True, c='ST_B6_Celsius', cmap=custom_palette, size=5, tiles='OSM', frame_width=700, frame_height=500, colorbar=True, clim=(20, 40))\n",
    "\n",
    "    # Plot square polygons with the same color mapping as the points\n",
    "    squares_plot = within_polygon_gdf.hvplot.polygons('geometry', c='ST_B6_Celsius', cmap=custom_palette, alpha=0.5, colorbar=True, clim=(20, 40))\n",
    "\n",
    "    # Plot the polygon with visible settings\n",
    "    polygon_plot = polygon_gdf.hvplot(geo=True, color='red', line_width=3, alpha=0.7)\n",
    "\n",
    "    # Overlay the polygon onto the heatmap\n",
    "    overlay_map = polygon_plot * heatmap * squares_plot\n",
    "\n",
    "    # Set up Panel to display the plot\n",
    "    # pane = pn.panel(overlay_map)\n",
    "\n",
    "    # pane.show()\n",
    "    # pane.save(f'C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\MSE-ES-UHI\\\\2_landsat\\\\Heatmaps\\\\{postal_code_112}_{satellite_image}_LST_Filtered.html', embed=True)\n",
    "\n",
    "    return overlay_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting LST over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining GDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing: L7_UTC_20220324_015557.tif\n",
      "Total number of valid pixels: 1409233\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 138\n",
      "Currently processing: L7_UTC_20220522_014715.tif\n",
      "Total number of valid pixels: 220421\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "Currently processing: L7_UTC_20220527_015249.tif\n",
      "Total number of valid pixels: 1492489\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 26\n",
      "Currently processing: L7_UTC_20220608_014623.tif\n",
      "Total number of valid pixels: 609784\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 138\n",
      "Currently processing: L7_UTC_20220712_014412.tif\n",
      "Total number of valid pixels: 75566\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "Currently processing: L7_UTC_20220717_014939.tif\n",
      "Total number of valid pixels: 310275\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "Currently processing: L7_UTC_20220729_014253.tif\n",
      "Total number of valid pixels: 806335\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 65\n",
      "Currently processing: L7_UTC_20220815_014126.tif\n",
      "Total number of valid pixels: 823955\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 59\n",
      "Currently processing: L7_UTC_20220901_013952.tif\n",
      "Total number of valid pixels: 1586707\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 138\n",
      "Currently processing: L7_UTC_20221022_013410.tif\n",
      "Total number of valid pixels: 1100227\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 122\n",
      "Currently processing: L7_UTC_20221027_013920.tif\n",
      "Total number of valid pixels: 274\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "Currently processing: L7_UTC_20221108_013155.tif\n",
      "Total number of valid pixels: 0\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "Currently processing: L7_UTC_20221113_013700.tif\n",
      "Total number of valid pixels: 13932\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "Currently processing: L7_UTC_20221217_013154.tif\n",
      "Total number of valid pixels: 51788\n",
      "   Latitude   Longitude\n",
      "0  1.470099  103.589751\n",
      "1  1.470099  103.590021\n",
      "2  1.470099  103.590290\n",
      "3  1.470099  103.590560\n",
      "4  1.470100  103.590830\n",
      "\n",
      "Number of points within 200m radius: 0\n",
      "          Longitude  Latitude    SR_B1    SR_B2    SR_B3    SR_B4    SR_B5  \\\n",
      "1273223  103.825765  1.276485  19893.0  21037.0  21179.0  24614.0  25101.0   \n",
      "1273224  103.826034  1.276485  19893.0  21894.0  22800.0  26305.0  27989.0   \n",
      "1273225  103.826304  1.276485  16643.0  18044.0  18850.0  22913.0  22648.0   \n",
      "1273226  103.826573  1.276485  13563.0  14088.0  14637.0  19287.0  17164.0   \n",
      "1273227  103.826843  1.276485  11988.0  12475.0  12315.0  18326.0  15253.0   \n",
      "...             ...       ...      ...      ...      ...      ...      ...   \n",
      "1291046  103.826575  1.273772   9891.0  10681.0  10459.0  12705.0  11848.0   \n",
      "1291047  103.826844  1.273772  10863.0  11261.0  11064.0  13786.0  12808.0   \n",
      "1291048  103.827114  1.273772  10650.0  11262.0  11165.0  14003.0  12807.0   \n",
      "1291049  103.827383  1.273772  10004.0  10683.0  10562.0  15077.0  12928.0   \n",
      "1291050  103.827653  1.273772   9896.0  10336.0  10258.0  16360.0  13407.0   \n",
      "\n",
      "           SR_B7  SR_ATMOS_OPACITY  SR_CLOUD_QA  ...  ST_ATRAN_Scaled  \\\n",
      "1273223  21939.0             306.0          2.0  ...           0.4470   \n",
      "1273224  23370.0             306.0          8.0  ...           0.4470   \n",
      "1273225  18831.0             306.0          8.0  ...           0.4470   \n",
      "1273226  13919.0             306.0          8.0  ...           0.4470   \n",
      "1273227  12239.0             306.0          8.0  ...           0.4470   \n",
      "...          ...               ...          ...  ...              ...   \n",
      "1291046  10232.0             469.0          NaN  ...           0.3481   \n",
      "1291047  11004.0             469.0          NaN  ...           0.3481   \n",
      "1291048  10747.0             468.0          NaN  ...           0.3481   \n",
      "1291049  10875.0             468.0          NaN  ...           0.3481   \n",
      "1291050  11004.0             467.0          NaN  ...           0.3481   \n",
      "\n",
      "         ST_CDIST_Scaled  ST_DRAD_Scaled  ST_EMIS_Scaled  ST_EMSD_Scaled  \\\n",
      "1273223             0.00           1.919          0.9397          0.0000   \n",
      "1273224             0.00           1.919          0.9737          0.0092   \n",
      "1273225             0.00           1.919          0.9745          0.0092   \n",
      "1273226             0.00           1.919          0.9760          0.0092   \n",
      "1273227             0.00           1.919          0.9782          0.0092   \n",
      "...                  ...             ...             ...             ...   \n",
      "1291046             0.11           2.085          0.9573          0.0128   \n",
      "1291047             0.13           2.085          0.9573          0.0128   \n",
      "1291048             0.15           2.085          0.9396          0.0128   \n",
      "1291049             0.17           2.085          0.9396          0.0128   \n",
      "1291050             0.20           2.085          0.9396          0.0128   \n",
      "\n",
      "         ST_QA_Scaled  ST_TRAD_Scaled  ST_URAD_Scaled  \\\n",
      "1273223          7.30        8.706000           4.464   \n",
      "1273224          7.34        8.744000           4.464   \n",
      "1273225          7.34        8.744000           4.464   \n",
      "1273226          7.36        8.706000           4.464   \n",
      "1273227          7.37        8.706000           4.464   \n",
      "...               ...             ...             ...   \n",
      "1291046          6.77        8.223001           4.923   \n",
      "1291047          6.68        8.223001           4.923   \n",
      "1291048          6.56        8.223001           4.923   \n",
      "1291049          6.45        8.223001           4.923   \n",
      "1291050          6.32        8.223001           4.923   \n",
      "\n",
      "                          geometry       time  \n",
      "1273223  POINT (103.82576 1.27648) 2022-03-24  \n",
      "1273224  POINT (103.82603 1.27649) 2022-03-24  \n",
      "1273225  POINT (103.82630 1.27649) 2022-03-24  \n",
      "1273226  POINT (103.82657 1.27649) 2022-03-24  \n",
      "1273227  POINT (103.82684 1.27649) 2022-03-24  \n",
      "...                            ...        ...  \n",
      "1291046  POINT (103.82657 1.27377) 2022-10-22  \n",
      "1291047  POINT (103.82684 1.27377) 2022-10-22  \n",
      "1291048  POINT (103.82711 1.27377) 2022-10-22  \n",
      "1291049  POINT (103.82738 1.27377) 2022-10-22  \n",
      "1291050  POINT (103.82765 1.27377) 2022-10-22  \n",
      "\n",
      "[686 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# Note that Landsat9 only has data from 2021 onwards\n",
    "year = \"2022\"\n",
    "\n",
    "# Suppress warnings\n",
    "logging.getLogger('bokeh').setLevel(logging.ERROR)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Specify the zip file and temporary directory for extraction\n",
    "zip_file_path = f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\Landsat7\\\\{year}.zip\"\n",
    "temp_dir = f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\temp_extract\"\n",
    "\n",
    "# Create a temporary directory if it doesn't exist\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Extract the .tif files from the zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "# Initialize an empty list to hold all the GeoDataFrames\n",
    "gdfs = []\n",
    "\n",
    "# Walk through the temporary directory and process each .tif file\n",
    "for filename in os.listdir(f\"{temp_dir}\\\\{year}\"):\n",
    "    if filename.endswith(\".tif\"):\n",
    "        print(\"Currently processing: \" + filename)\n",
    "        file_path = os.path.join(f\"{temp_dir}\\\\{year}\", filename)\n",
    "        \n",
    "        # Extract the time period from the filename\n",
    "        # Assuming filename format is \"L8_UTC_YYYYMMDD_hhmmss.tif\"\n",
    "        time_str = filename.split('_')[2]\n",
    "        time_obj = datetime.strptime(time_str, \"%Y%m%d\")\n",
    "        \n",
    "        # Load and preprocess the GeoDataFrame\n",
    "        gdf = preprocessing(file_path)\n",
    "        gdf['time'] = time_obj  # Append the datetime object as a new column\n",
    "        \n",
    "        # Append the processed GeoDataFrame to the list\n",
    "        gdfs.append(gdf)\n",
    "\n",
    "# Combine all GeoDataFrames into one\n",
    "combined_gdf = pd.concat(gdfs)\n",
    "\n",
    "shutil.rmtree(f\"C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\temp_extract\")\n",
    "\n",
    "# Use the combined GeoDataFrame as needed\n",
    "print(combined_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spatial plot over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying plot for 2020-04-19\n",
      "Number of pixels in region of interest: 33\n",
      "Launching server at http://localhost:64630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<panel.io.server.Server at 0x2169359d3c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an interactive plot with filtering based on the GeoDataFrame\n",
    "def create_interactive_plot(combined_gdf):\n",
    "    # Create a list of unique dates sorted\n",
    "    unique_dates = combined_gdf['time'].dt.strftime('%Y-%m-%d').sort_values().unique()\n",
    "    # print(f\"Unique Dates: {unique_dates}\")\n",
    "\n",
    "    date_index_map = {i + 1: date for i, date in enumerate(unique_dates)}\n",
    "\n",
    "    # Setup an integer slider to select time periods\n",
    "    time_slider = pn.widgets.IntSlider(name='Select Time', start=1, end=len(unique_dates), value=1, step=1)\n",
    "\n",
    "    @pn.depends(time_slider.param.value_throttled)\n",
    "    def dynamic_map(value):\n",
    "        selected_date = date_index_map[value]\n",
    "        selected_datetime = pd.to_datetime(selected_date).date()\n",
    "        \n",
    "        # Filter data for the selected time\n",
    "        filtered_data = combined_gdf[combined_gdf['time'].dt.date == selected_datetime]\n",
    "        print(f\"Displaying plot for \" + str(selected_date))\n",
    "        \n",
    "        # Call plot_spatial_map for the selected time period\n",
    "        return plot_spatial_map(filtered_data)\n",
    "\n",
    "    layout = pn.Column(\n",
    "        \"<br>\\nInteractive Land Surface Temperature Map\",\n",
    "        time_slider,\n",
    "        dynamic_map\n",
    "    )\n",
    "\n",
    "    return layout\n",
    "\n",
    "layout = create_interactive_plot(combined_gdf)\n",
    "# layout.servable()\n",
    "pn.serve(layout, show=False, start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to C:\\LocalOneDrive\\Documents\\Desktop\\MTI\\UHI-Project\\MSE-ES-UHI\\Data\\FilteredData\\BukitPurmei\\BukitPurmei_Filtered_2019.csv\n"
     ]
    }
   ],
   "source": [
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# from shapely.geometry import box\n",
    "\n",
    "# def filter_and_save_data(year_gdf, polygons, output_file):\n",
    "#     # Ensure polygons are in EPSG:3857\n",
    "#     polygon_gdf = gpd.GeoDataFrame({'geometry': list(polygons.values())}, crs='epsg:4326')\n",
    "#     polygon_gdf = polygon_gdf.to_crs('epsg:3857')\n",
    "\n",
    "#     # Initialize an empty DataFrame to store all filtered data\n",
    "#     all_filtered_data = gpd.GeoDataFrame()\n",
    "\n",
    "#     for date in year_gdf['time'].dt.strftime('%Y-%m-%d').sort_values().unique():\n",
    "#         # Filter data for the specific date\n",
    "#         date_data = year_gdf[year_gdf['time'].dt.strftime('%Y-%m-%d') == date]\n",
    "\n",
    "#         # Convert CRS to EPSG:3857 and create 30m x 30m boxes around each point\n",
    "#         date_data = date_data.to_crs('epsg:3857')\n",
    "#         date_data['geometry'] = date_data['geometry'].apply(\n",
    "#             lambda x: box(x.x - 15, x.y - 15, x.x + 15, x.y + 15))\n",
    "\n",
    "#         # Filter points that intersect any polygon\n",
    "#         date_data['intersects'] = date_data['geometry'].apply(\n",
    "#             lambda geom: any(geom.intersects(poly) for poly in polygon_gdf['geometry']))\n",
    "#         filtered_data = date_data[date_data['intersects']].copy()\n",
    "\n",
    "#         # Append the filtered data of this date to the all_filtered_data DataFrame\n",
    "#         all_filtered_data = pd.concat([all_filtered_data, filtered_data], ignore_index=True)\n",
    "\n",
    "#     # Drop the 'geometry' column as it cannot be saved directly in CSV format\n",
    "#     all_filtered_data.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "#     # Save the aggregated filtered data to a CSV file\n",
    "#     all_filtered_data.to_csv(output_file, index=False)\n",
    "#     print(f\"Data successfully exported to {output_file}\")\n",
    "\n",
    "# combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])  # Ensure 'time' is a datetime object\n",
    "# output_path = 'C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\FilteredData\\\\BukitPurmei\\\\BukitPurmei_Filtered_2019.csv'\n",
    "# filter_and_save_data(combined_gdf, polygons, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Longitude  Latitude    SR_B1    SR_B2    SR_B3    SR_B4    SR_B5  \\\n",
      "0    103.826035  1.275400  10861.0  11662.0  11596.0  16780.0  13677.0   \n",
      "1    103.826304  1.275400  11426.0  11967.0  11596.0  16393.0  14015.0   \n",
      "2    103.826574  1.275400  12825.0  13787.0  13836.0  17940.0  16153.0   \n",
      "3    103.826843  1.275400  16911.0  17850.0  18067.0  21583.0  20750.0   \n",
      "4    103.827113  1.275400  19287.0  20560.0  21180.0  23860.0  23987.0   \n",
      "..          ...       ...      ...      ...      ...      ...      ...   \n",
      "152  103.827383  1.274586  10324.0  10449.0  10357.0  12489.0  11487.0   \n",
      "153  103.825496  1.274314  10533.0  11489.0  11564.0  15296.0  12450.0   \n",
      "154  103.825765  1.274314  11820.0  12410.0  12367.0  16152.0  13410.0   \n",
      "155  103.826035  1.274314  11715.0  12524.0  12567.0  15295.0  13648.0   \n",
      "156  103.826305  1.274314  10752.0  11605.0  11365.0  14219.0  12329.0   \n",
      "\n",
      "       SR_B7  SR_ATMOS_OPACITY  SR_CLOUD_QA  ...  ST_ATRAN_Scaled  \\\n",
      "0    11399.0             306.0          8.0  ...           0.4470   \n",
      "1    11519.0             306.0          8.0  ...           0.4470   \n",
      "2    13800.0             306.0          8.0  ...           0.4470   \n",
      "3    18592.0             306.0          2.0  ...           0.4470   \n",
      "4    21342.0             306.0          2.0  ...           0.4470   \n",
      "..       ...               ...          ...  ...              ...   \n",
      "152   9845.0             469.0          NaN  ...           0.3481   \n",
      "153   9974.0             472.0          NaN  ...           0.3481   \n",
      "154  10877.0             471.0          NaN  ...           0.3481   \n",
      "155  11392.0             471.0          NaN  ...           0.3481   \n",
      "156  10747.0             470.0          NaN  ...           0.3481   \n",
      "\n",
      "     ST_CDIST_Scaled  ST_DRAD_Scaled  ST_EMIS_Scaled  ST_EMSD_Scaled  \\\n",
      "0               0.00           1.919          0.9457          0.0011   \n",
      "1               0.00           1.919          0.9457          0.0011   \n",
      "2               0.00           1.919          0.9457          0.0011   \n",
      "3               0.00           1.919          0.9457          0.0011   \n",
      "4               0.00           1.919          0.9574          0.0070   \n",
      "..               ...             ...             ...             ...   \n",
      "152             0.13           2.085          0.9607          0.0043   \n",
      "153             0.03           2.085          0.9620          0.0000   \n",
      "154             0.03           2.085          0.9620          0.0000   \n",
      "155             0.03           2.085          0.9646          0.0056   \n",
      "156             0.04           2.085          0.9646          0.0056   \n",
      "\n",
      "     ST_QA_Scaled  ST_TRAD_Scaled  ST_URAD_Scaled       time        block  \n",
      "0            7.19        8.967000           4.464 2022-03-24  polygon_114  \n",
      "1            7.24        8.855000           4.464 2022-03-24  polygon_114  \n",
      "2            7.27        8.781000           4.464 2022-03-24  polygon_115  \n",
      "3            7.30        8.706000           4.464 2022-03-24  polygon_115  \n",
      "4            7.35        8.669001           4.464 2022-03-24  polygon_115  \n",
      "..            ...             ...             ...        ...          ...  \n",
      "152          6.64        8.185000           4.923 2022-10-22  polygon_113  \n",
      "153          7.13        8.185000           4.923 2022-10-22  polygon_112  \n",
      "154          7.13        8.185000           4.923 2022-10-22  polygon_112  \n",
      "155          7.12        8.223001           4.923 2022-10-22  polygon_112  \n",
      "156          7.06        8.223001           4.923 2022-10-22  polygon_112  \n",
      "\n",
      "[157 rows x 38 columns]\n",
      "Data successfully exported to C:\\LocalOneDrive\\Documents\\Desktop\\MTI\\UHI-Project\\MSE-ES-UHI\\Data\\FilteredData\\BukitPurmei\\Landsat7\\BukitPurmei_Filtered_2022_Blocks.csv\n"
     ]
    }
   ],
   "source": [
    "def filter_and_save_data(year_gdf, polygons, output_file):\n",
    "    # Convert the polygons dictionary to a GeoDataFrame\n",
    "    polygon_gdf = gpd.GeoDataFrame({\n",
    "        'block': list(polygons.keys()),   # Keys from your dictionary\n",
    "        'geometry': list(polygons.values())\n",
    "    }, crs='epsg:4326')\n",
    "    polygon_gdf = polygon_gdf.to_crs('epsg:3857')\n",
    "\n",
    "    # Initialize an empty DataFrame to store all filtered data\n",
    "    all_filtered_data = gpd.GeoDataFrame()\n",
    "\n",
    "    for date in year_gdf['time'].dt.strftime('%Y-%m-%d').sort_values().unique():\n",
    "        # Filter data for the specific date\n",
    "        date_data = year_gdf[year_gdf['time'].dt.strftime('%Y-%m-%d') == date]\n",
    "\n",
    "        # Convert CRS to EPSG:3857 and create 30m x 30m boxes around each point\n",
    "        date_data = date_data.to_crs('epsg:3857')\n",
    "        date_data['geometry'] = date_data['geometry'].apply(\n",
    "            lambda x: box(x.x - 15, x.y - 15, x.x + 15, x.y + 15))\n",
    "\n",
    "        # Determine the block for each point by finding which polygon it intersects\n",
    "        def find_block(geom):\n",
    "            for idx, poly in polygon_gdf.iterrows():\n",
    "                if geom.intersects(poly['geometry']):\n",
    "                    return poly['block']\n",
    "            return None  # Return None or an appropriate value if no intersection is found\n",
    "\n",
    "        date_data['block'] = date_data['geometry'].apply(find_block)\n",
    "\n",
    "        # Filter to keep only data that intersects with a polygon\n",
    "        filtered_data = date_data[date_data['block'].notnull()].copy()\n",
    "\n",
    "        # Append the filtered data of this date to the all_filtered_data DataFrame\n",
    "        all_filtered_data = pd.concat([all_filtered_data, filtered_data], ignore_index=True)\n",
    "\n",
    "    # Drop the 'geometry' column as it cannot be saved directly in CSV format\n",
    "    all_filtered_data.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "    print(all_filtered_data)\n",
    "\n",
    "    # Save the aggregated filtered data to a CSV file\n",
    "    all_filtered_data.to_csv(output_file, index=False)\n",
    "    print(f\"Data successfully exported to {output_file}\")\n",
    "\n",
    "# Example usage, assuming combined_gdf and polygons are defined earlier\n",
    "combined_gdf['time'] = pd.to_datetime(combined_gdf['time'])  # Ensure 'time' is a datetime object\n",
    "output_path = 'C:\\\\LocalOneDrive\\\\Documents\\\\Desktop\\\\MTI\\\\UHI-Project\\\\MSE-ES-UHI\\\\Data\\\\FilteredData\\\\BukitPurmei\\\\Landsat7\\\\BukitPurmei_Filtered_2022_Blocks.csv'\n",
    "filter_and_save_data(combined_gdf, polygons, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codes to combine data from 2019 to 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files were successfully concatenated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the base file path\n",
    "base_path = r\"C:\\LocalOneDrive\\Documents\\Desktop\\MTI\\UHI-Project\\MSE-ES-UHI\\Data\\FilteredData\\BukitPurmei\\Landsat7\"\n",
    "\n",
    "# File names\n",
    "files = [\n",
    "    r\"BukitPurmei_Filtered_2018_Blocks.csv\",\n",
    "    r\"BukitPurmei_Filtered_2019_to_2021_Blocks.csv\",\n",
    "    r\"BukitPurmei_Filtered_2022_Blocks.csv\"\n",
    "]\n",
    "\n",
    "# Read and concatenate the CSV files\n",
    "df_list = [pd.read_csv(f\"{base_path}\\\\{file_name}\") for file_name in files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(f\"{base_path}\\\\BukitPurmei_Filtered_2018_to_2022_Blocks.csv\", index=False)\n",
    "\n",
    "print(\"Files were successfully concatenated and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
